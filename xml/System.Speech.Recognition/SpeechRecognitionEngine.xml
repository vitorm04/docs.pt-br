<Type Name="SpeechRecognitionEngine" FullName="System.Speech.Recognition.SpeechRecognitionEngine">
  <TypeSignature Language="C#" Value="public class SpeechRecognitionEngine : IDisposable" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi beforefieldinit SpeechRecognitionEngine extends System.Object implements class System.IDisposable" />
  <TypeSignature Language="DocId" Value="T:System.Speech.Recognition.SpeechRecognitionEngine" />
  <AssemblyInfo>
    <AssemblyName>System.Speech</AssemblyName>
    <AssemblyVersion>4.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>System.Object</BaseTypeName>
  </Base>
  <Interfaces>
    <Interface>
      <InterfaceName>System.IDisposable</InterfaceName>
    </Interface>
  </Interfaces>
  <Docs>
    <summary>Fornece os meios de acessar e gerenciar um mecanismo de reconhecimento de fala no processo.</summary>
    <remarks>
      <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Você pode criar uma instância dessa classe para qualquer um dos identificadores de fala instalados. Para obter informações sobre quais identificadores são instalados, use estático <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> método.  
  
 Essa classe é para fala reconhecimento mecanismos em processo e fornece controle sobre diversos aspectos de reconhecimento de fala, da seguinte maneira:  
  
-   Para criar um reconhecedor de fala em processo, use um do <xref:System.Speech.Recognition.SpeechRecognitionEngine.%23ctor%2A> construtores.  
  
-   Para gerenciar as gramáticas de reconhecimento de fala, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> métodos e o <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> propriedade.  
  
-   Para configurar a entrada para o reconhecedor, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>, ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A> método.  
  
-   Para executar o reconhecimento de fala, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> método.  
  
-   Para modificar como reconhecimento trata silêncio ou inesperado de entrada, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriedades.  
  
-   Para alterar o número de alternativas que retorna o reconhecedor, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> propriedade. O reconhecedor retorna resultados de reconhecimento em um <xref:System.Speech.Recognition.RecognitionResult> objeto.  
  
-   Para sincronizar as alterações para o reconhecedor, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> método. O reconhecedor usa mais de um thread para executar tarefas.  
  
-   Para emular o reconhecedor de entrada, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> métodos.  
  
 O <xref:System.Speech.Recognition.SpeechRecognitionEngine> objeto é para usa a única do processo que criar uma instância do objeto. Por outro lado, o <xref:System.Speech.Recognition.SpeechRecognizer> compartilha um identificador único com qualquer aplicativo que deseja usá-lo.  
  
> [!NOTE]
>  Sempre chamar <xref:System.Speech.Recognition.SpeechRecognitionEngine.Dispose%2A> antes de liberar sua última referência para o reconhecedor de fala. Caso contrário, os recursos que está usando não serão liberados até que o coletor de lixo chama o objeto de reconhecedor `Finalize` método.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala básico. Como este exemplo usa o `Multiple` modo do <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> método, ele executa o reconhecimento até você fechar a janela do console ou parar a depuração.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
    </remarks>
  </Docs>
  <Members>
    <MemberGroup MemberName=".ctor">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Inicializa uma nova instância da classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Você pode construir um <xref:System.Speech.Recognition.SpeechRecognitionEngine> instância de qualquer um dos seguintes:  
  
-   O mecanismo de reconhecimento de fala padrão para o sistema  
  
-   Um mecanismo de reconhecimento de fala específico que você especifica por nome  
  
-   O mecanismo de reconhecimento de fala padrão para uma localidade que você especificar  
  
-   Um mecanismo de reconhecimento específicos que atende aos critérios que você especificar em uma <xref:System.Speech.Recognition.RecognizerInfo> objeto.  
  
 Antes de iniciar o reconhecimento do reconhecedor de fala, você deve carregar a gramática de reconhecimento de fala de pelo menos um e configurar a entrada do reconhecedor.  
  
 Para carregar uma gramática, chame o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
 Para configurar a entrada de áudio, use um dos seguintes métodos:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters />
      <Docs>
        <summary>Inicializa uma nova instância da classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> usando o reconhecedor de fala padrão do sistema.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Antes de iniciar o reconhecimento de fala o reconhecedor de fala, você deve carregar pelo menos uma gramática de reconhecimento e configurar a entrada do reconhecedor.  
  
 Para carregar uma gramática, chame o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
 Para configurar a entrada de áudio, use um dos seguintes métodos:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Globalization.CultureInfo culture);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Globalization.CultureInfo culture) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Globalization.CultureInfo)" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="culture" Type="System.Globalization.CultureInfo" />
      </Parameters>
      <Docs>
        <param name="culture">A localidade para a qual o reconhecedor de fala deve dar suporte.</param>
        <summary>Inicializa uma nova instância da classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> usando o reconhecedor de fala padrão de uma localidade específica.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Microsoft Windows e a API Speech aceitar todos os códigos de país de idioma válidos. Para executar o reconhecimento de fala usando o idioma especificado no `CultureInfo` argumento, um mecanismo de reconhecimento de fala que dá suporte a que o código do país de idioma deve ser instalado. Os mecanismos de reconhecimento de fala fornecido com o Microsoft Windows 7 trabalhar com os seguintes códigos de país de idioma.  
  
-   en-GB. Inglês (Reino Unido)  
  
-   en-US. Inglês (Estados Unidos)  
  
-   de-DE. Alemão (Alemanha)  
  
-   es-ES. Espanhol (Espanha)  
  
-   fr-FR. Francês (França)  
  
-   ja-JP. Japonês (Japão)  
  
-   zh-CN. Chinês (China)  
  
-   zh-TW. Chinês (Taiwan)  
  
 Os códigos de idioma com duas letras, como "en", "fr" ou "es" também são permitidos.  
  
 Antes de iniciar o reconhecimento do reconhecedor de fala, você deve carregar a gramática de reconhecimento de fala de pelo menos um e configurar a entrada do reconhecedor.  
  
 Para carregar uma gramática, chame o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
 Para configurar a entrada de áudio, use um dos seguintes métodos:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala básico e inicializa um reconhecedor de fala para a localidade en-US.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Nenhum dos reconhecedores de fala instalados dão suporte à localidade especificada, ou <paramref name="culture" /> é a cultura invariável.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Culture" /> é <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (System.Speech.Recognition.RecognizerInfo recognizerInfo);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(class System.Speech.Recognition.RecognizerInfo recognizerInfo) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.Speech.Recognition.RecognizerInfo)" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerInfo" Type="System.Speech.Recognition.RecognizerInfo" />
      </Parameters>
      <Docs>
        <param name="recognizerInfo">As informações para o reconhecedor de fala específico.</param>
        <summary>Inicializa uma nova instância do <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> usando as informações em um objeto <see cref="T:System.Speech.Recognition.RecognizerInfo" /> para especificar o reconhecedor a ser usado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Você pode criar uma instância dessa classe para qualquer um dos identificadores de fala instalados. Para obter informações sobre quais identificadores são instalados, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> método.  
  
 Antes de iniciar o reconhecimento do reconhecedor de fala, você deve carregar a gramática de reconhecimento de fala de pelo menos um e configurar a entrada do reconhecedor.  
  
 Para carregar uma gramática, chame o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
 Para configurar a entrada de áudio, use um dos seguintes métodos:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala básico e inicializa um reconhecedor de fala que dá suporte ao idioma inglês.  
  
```csharp  
 using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName=".ctor">
      <MemberSignature Language="C#" Value="public SpeechRecognitionEngine (string recognizerId);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig specialname rtspecialname instance void .ctor(string recognizerId) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.#ctor(System.String)" />
      <MemberType>Constructor</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Parameters>
        <Parameter Name="recognizerId" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="recognizerId">O nome do token do reconhecedor de fala a ser usado.</param>
        <summary>Inicializa uma nova instância da classe <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> com um parâmetro de cadeia de caracteres que especifica o nome do reconhecedor a ser usado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O nome do token do reconhecedor de é o valor do <xref:System.Speech.Recognition.RecognizerInfo.Id%2A> propriedade o <xref:System.Speech.Recognition.RecognizerInfo> objeto retornado pelo <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> propriedade do reconhecedor de. Para obter uma coleção de todos os identificadores instalados, use estático <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> método.  
  
 Antes de iniciar o reconhecimento do reconhecedor de fala, você deve carregar a gramática de reconhecimento de fala de pelo menos um e configurar a entrada do reconhecedor.  
  
 Para carregar uma gramática, chame o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
 Para configurar a entrada de áudio, use um dos seguintes métodos:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala básico e cria uma instância do 8.0 do reconhecedor de fala para Windows (inglês - EUA).  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an instance of the Microsoft Speech Recognizer 8.0 for  
      // Windows (English - US).  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine("MS-1033-80-DESK"))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized += new EventHandler(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentException">Nenhum reconhecedor de fala com esse nome de token está instalado ou <paramref name="recognizerId" /> é a cadeia de caracteres vazia ("").</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="recognizerId" /> é <see langword="null" />.</exception>
      </Docs>
    </Member>
    <Member MemberName="AudioFormat">
      <MemberSignature Language="C#" Value="public System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.AudioFormat.SpeechAudioFormatInfo AudioFormat" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.AudioFormat.SpeechAudioFormatInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtém o formato de áudio sendo recebido pelo <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>O formato de áudio na entrada para a instância <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> ou <see langword="null" /> se a entrada não estiver configurada ou definida como a entrada nula.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Para configurar a entrada de áudio, use um dos seguintes métodos:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile%2A>  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream%2A>  
  
   
  
## Examples  
 O exemplo a seguir usa <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioFormat%2A> para obter e exibir dados de formato de áudio.  
  
```  
static void DisplayAudioDeviceFormat(Label label, SpeechRecognitionEngine recognitionEngine)   
{  
  
  if (recognitionEngine != null && label != null)   
  {  
    label.Text = String.Format("Encoding Format:         {0}\n" +  
          "AverageBytesPerSecond    {1}\n" +  
          "BitsPerSample            {2}\n" +  
          "BlockAlign               {3}\n" +  
          "ChannelCount             {4}\n" +  
          "SamplesPerSecond         {5}",  
          recognitionEngine.AudioFormat.EncodingFormat.ToString(),  
          recognitionEngine.AudioFormat.AverageBytesPerSecond,  
          recognitionEngine.AudioFormat.BitsPerSample,  
          recognitionEngine.AudioFormat.BlockAlign,  
          recognitionEngine.AudioFormat.ChannelCount,  
          recognitionEngine.AudioFormat.SamplesPerSecond);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioLevel">
      <MemberSignature Language="C#" Value="public int AudioLevel { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 AudioLevel" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtém o nível do áudio sendo recebido pelo <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>O nível do áudio da entrada para o reconhecedor de fala, de 0 a 100.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O valor 0 representa silêncio e 100 representa o volume máximo de entrada.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioLevelUpdated">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt; AudioLevelUpdated" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioLevelUpdatedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Gerado quando o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> relata o nível de sua entrada de áudio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O <xref:System.Speech.Recognition.SpeechRecognitionEngine> gera esse evento várias vezes por segundo. A frequência com que o evento é gerado depende do computador no qual o aplicativo está sendo executado.  
  
 Para obter o nível de áudio no momento do evento, use o <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs.AudioLevel%2A> propriedade associado <xref:System.Speech.Recognition.AudioLevelUpdatedEventArgs>. Para obter o nível atual de áudio de entrada para o reconhecedor, use o reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> propriedade.  
  
 Quando você cria um <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> delegado, você identificar o método que manipulará o evento. Para associar o evento com o manipulador de eventos, adicione uma instância do representante ao evento. O manipulador de eventos é chamado sempre que o evento ocorre, a menos que você remova o representante. Para obter mais informações sobre delegados de manipulador de eventos, consulte [eventos e delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 O exemplo a seguir adiciona um manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevelUpdated> evento para um <xref:System.Speech.Recognition.SpeechRecognitionEngine> objeto. O manipulador gera o novo nível de áudio para o console.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the SpeechRecognitionEngine object.   
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add an event handler for the AudioLevelUpdated event.  
  recognizer.AudioLevelUpdated +=   
   new EventHandler<AudioLevelUpdatedEventArgs>(recognizer_AudioLevelUpdated);  
  
  // Add other initialization code here.  
  
}  
  
// Write the audio level to the console when the AudioLevelUpdated event is raised.  
void recognizer_AudioLevelUpdated(object sender, AudioLevelUpdatedEventArgs e)  
{  
  Console.WriteLine("The audio level is now: {0}.", e.AudioLevel);  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan AudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan AudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtém o local atual no fluxo de áudio que está sendo gerado pelo dispositivo que está fornecendo a entrada para o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>O local atual no fluxo de áudio que está sendo gerado pelo dispositivo de entrada.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propriedade faz referência a posição do dispositivo de entrada no seu fluxo de áudio gerado. Por outro lado, o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> propriedade faz referência a posição do reconhecedor dentro de sua entrada de áudio. Essas posições podem ser diferentes. Por exemplo, se tiver recebido o reconhecedor de entrada para o qual não tem ainda gerado um resultado de reconhecimento e o valor da <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> propriedade é menor que o valor da <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propriedade.  
  
   
  
## Examples  
 No exemplo a seguir, o reconhecedor de fala em processo usa uma gramática ditado para corresponder à entrada de fala. Um manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> evento grava no console de <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioLevel%2A> quando detecta que o reconhecedor de fala fala em sua entrada.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine for US English.  
      using (recognizer = new SpeechRecognitionEngine(  
        new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create a grammar for finding services in different cities.  
        Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
        Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
        GrammarBuilder findServices = new GrammarBuilder("Find");  
        findServices.Append(services);  
        findServices.Append("near");  
        findServices.Append(cities);  
  
        // Create a Grammar object from the GrammarBuilder and load it to the recognizer.  
        Grammar servicesGrammar = new Grammar(findServices);  
        recognizer.LoadGrammarAsync(servicesGrammar);  
  
        // Add handlers for events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
        Console.WriteLine("Starting asynchronous recognition...");  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Gather information about detected speech and write it to the console.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Speech detected:");  
      Console.WriteLine("  Audio level: " + recognizer.AudioLevel);  
      Console.WriteLine("  Audio position at the event: " + e.AudioPosition);  
      Console.WriteLine("  Current audio position: " + recognizer.AudioPosition);  
      Console.WriteLine("  Current recognizer audio position: " +   
        recognizer.RecognizerAudioPosition);  
    }  
  
    // Write the text of the recognition result to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("\nSpeech recognized: " + e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioSignalProblemOccurred">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt; AudioSignalProblemOccurred" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioSignalProblemOccurredEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Gerado quando o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> detecta um problema no sinal de áudio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Para obter o problema, use o <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs.AudioSignalProblem%2A> propriedade associado <xref:System.Speech.Recognition.AudioSignalProblemOccurredEventArgs>.  
  
 Quando você cria um <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> delegado, você identificar o método que manipulará o evento. Para associar o evento com o manipulador de eventos, adicione uma instância do representante ao evento. O manipulador de eventos é chamado sempre que o evento ocorre, a menos que você remova o representante. Para obter mais informações sobre delegados de manipulador de eventos, consulte [eventos e delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 O exemplo a seguir define um manipulador de eventos que coleta informações sobre um <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioSignalProblemOccurred> eventos.  
  
```  
private SpeechRecognitionEngine recognizer;  
  
// Initialize the speech recognition engine.  
private void Initialize()  
{  
  recognizer = new SpeechRecognitionEngine();  
  
  // Add a handler for the AudioSignalProblemOccurred event.  
  recognizer.AudioSignalProblemOccurred +=   
    new EventHandler<AudioSignalProblemOccurredEventArgs>(  
      recognizer_AudioSignalProblemOccurred);  
}  
  
// Gather information when the AudioSignalProblemOccurred event is raised.  
void recognizer_AudioSignalProblemOccurred(object sender, AudioSignalProblemOccurredEventArgs e)  
{  
  StringBuilder details = new StringBuilder();  
  
  details.AppendLine("Audio signal problem information:");  
  details.AppendFormat(  
    " Audio level:               {0}" + Environment.NewLine +  
    " Audio position:            {1}" + Environment.NewLine +  
    " Audio signal problem:      {2}" + Environment.NewLine +  
    " Recognition engine audio position: {3}" + Environment.NewLine,  
    e.AudioLevel, e.AudioPosition,  e.AudioSignalProblem,  
    e.recoEngineAudioPosition);  
  
  // Insert additional event handler code here.  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioState">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.AudioState AudioState { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.Speech.Recognition.AudioState AudioState" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioState" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.AudioState</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtém o estado do áudio sendo recebido pelo <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>O estado da entrada de áudio para o reconhecedor de fala.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> propriedade representa o estado de áudio com um membro do <xref:System.Speech.Recognition.AudioState> enumeração.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="AudioStateChanged">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.AudioStateChangedEventArgs&gt; AudioStateChanged" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.AudioStateChangedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Gerado quando as alterações de estado no áudio sendo recebidos pelo <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Para obter o estado de áudio no momento do evento, use o <xref:System.Speech.Recognition.AudioStateChangedEventArgs.AudioState%2A> propriedade associado <xref:System.Speech.Recognition.AudioStateChangedEventArgs>. Para obter o estado atual de áudio de entrada para o reconhecedor, use o reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> propriedade. Para obter mais informações sobre o estado de áudio, consulte o <xref:System.Speech.Recognition.AudioState> enumeração.  
  
 Quando você cria um <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> delegado, você identificar o método que manipulará o evento. Para associar o evento com o manipulador de eventos, adicione uma instância do representante ao evento. O manipulador de eventos é chamado sempre que o evento ocorre, a menos que você remova o representante. Para obter mais informações sobre delegados de manipulador de eventos, consulte [eventos e delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 O exemplo a seguir usa um manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> do novo evento ao gravar o reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioState%2A> para o console sempre que as alterações, usando um membro do <xref:System.Speech.Recognition.AudioState> enumeração.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder("On this farm he had a");  
        farm.Append(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Attach event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(recognizer_AudioStateChanged);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine();  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Done.");  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the AudioStateChanged event.  
    static void recognizer_AudioStateChanged(object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("The new audio state is: " + e.AudioState);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="BabbleTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan BabbleTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan BabbleTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtém ou define o intervalo de tempo durante o qual um <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> aceita entrada contendo apenas ruídos de fundo, antes de finalizar o reconhecimento.</summary>
        <value>A duração do intervalo de tempo.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cada reconhecedor de fala tem um algoritmo para distinguir entre silêncio e fala. O reconhecedor classifica como ruídos de fundo qualquer não silêncio de entrada que não correspondem à regra inicial de qualquer um do reconhecedor carregados e habilitados gramáticas de reconhecimento de fala. Se o reconhecedor recebe apenas ruídos de fundo e silêncio dentro do intervalo de tempo limite de Diafonia Múltipla, o reconhecedor finaliza a operação reconhecimento.  
  
-   Para operações assíncronas de reconhecimento, gera o reconhecedor de <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, onde o <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A?displayProperty=nameWithType> é de propriedade `true`e o <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> é de propriedade `null`.  
  
-   Para operações síncronas reconhecimento e emulação, retorna o reconhecedor `null`, em vez de uma opção válida <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Se o período de tempo limite de Diafonia Múltipla é definido como 0, o reconhecedor não executa uma verificação de tempo limite de Diafonia Múltipla. O intervalo de tempo limite pode ser qualquer valor negativo. O padrão é 0 segundos.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala básica que define o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriedades de um <xref:System.Speech.Recognition.SpeechRecognitionEngine> antes de iniciar o reconhecimento de fala. Manipuladores para o reconhecedor de fala <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos de saída de informações de evento para o console para demonstrar como o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriedades de um <xref:System.Speech.Recognition.SpeechRecognitionEngine> afetam as operações de reconhecimento.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Esta propriedade está definida como menos de 0 segundos.</exception>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="public void Dispose ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig newslot virtual instance void Dispose() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Descarta o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> objeto.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <Member MemberName="Dispose">
      <MemberSignature Language="C#" Value="protected virtual void Dispose (bool disposing);" />
      <MemberSignature Language="ILAsm" Value=".method familyhidebysig newslot virtual instance void Dispose(bool disposing) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Dispose(System.Boolean)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="disposing" Type="System.Boolean" />
      </Parameters>
      <Docs>
        <param name="disposing">
          <see langword="true" /> para liberar recursos gerenciados e não gerenciados; <see langword="false" /> para liberar apenas recursos não gerenciados.</param>
        <summary>Descarta o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> recursos de objeto e as versões usados durante a sessão.</summary>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emula a entrada para o reconhecedor de fala, usando texto em vez de áudio para o reconhecimento de fala síncrona.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Esses métodos ignorar a entrada de áudio do sistema e fornecer o texto para o reconhecedor como <xref:System.String> objetos ou como uma matriz de <xref:System.Speech.Recognition.RecognizedWordUnit> objetos. Isso pode ser útil quando você estiver testando ou depurar um aplicativo ou gramática. Por exemplo, você pode usar emulação para determinar se uma palavra está em uma gramática e quais semântica é retornada quando a palavra é reconhecida. Use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> método para desabilitar a entrada de áudio para o mecanismo de reconhecimento de voz durante as operações de emulação.  
  
 A gera do reconhecedor de fala a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como se a operação de reconhecimento não é emulada. O reconhecedor ignora novas linhas e espaço em branco extra e trata pontuação como entrada literal.  
  
> [!NOTE]
>  O <xref:System.Speech.Recognition.RecognitionResult> objeto gerado pelo reconhecedor de fala em resposta à entrada emulada tem um valor de `null` para sua <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> propriedade.  
  
 Para emular reconhecimento assíncrono, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> método.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">A entrada para a operação de reconhecimento.</param>
        <summary>Emula a entrada de uma frase para o reconhecedor de fala, usando texto em vez de áudio para o reconhecimento de fala síncrona.</summary>
        <returns>O resultado da operação de reconhecimento, ou <see langword="null" /> se a operação não for bem-sucedida ou o reconhecedor não está habilitado.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 A gera do reconhecedor de fala a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como se a operação de reconhecimento não é emulada.  
  
 Os identificadores que acompanham o Vista e Windows 7 Ignorar maiusculas e minúsculas e largura de caracteres ao aplicar regras de gramática para a expressão de entrada. Para obter mais informações sobre esse tipo de comparação, consulte o <xref:System.Globalization.CompareOptions> valores de enumeração <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> e <xref:System.Globalization.CompareOptions.IgnoreWidth>. Os identificadores também ignoram linhas novas e espaço em branco extra e tratam a pontuação como entrada literal.  
  
   
  
## Examples  
 O exemplo de código a seguir é parte de um aplicativo de console que demonstra a entrada emulada, os resultados do reconhecimento associados e os eventos associados emitidos o reconhecedor de fala. O exemplo gera a saída a seguir.  
  
```  
TestRecognize("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
...Recognition result text = Smith  
  
TestRecognize("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
...Recognition result text = Jones  
  
TestRecognize("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
...No recognition result.  
  
TestRecognize("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
...Recognition result text = mister Smith  
  
press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace Sre_EmulateRecognize  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Disable audio input to the recognizer.  
        recognizer.SetInputToNull();  
  
        // Add handlers for events raised by the EmulateRecognize method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
  
        // Start four synchronous emulated recognition operations.  
        TestRecognize(recognizer, "Smith");  
        TestRecognize(recognizer, "Jones");  
        TestRecognize(recognizer, "Mister");  
        TestRecognize(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for synchronous recognition.  
    private static void TestRecognize(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      Console.WriteLine("TestRecognize(\"{0}\")...", input);  
      RecognitionResult result =  
        recognizer.EmulateRecognize(input,CompareOptions.IgnoreCase);  
      if (result != null)  
      {  
        Console.WriteLine("...Recognition result text = {0}",  
          result.Text ?? "<null>");  
      }  
      else  
      {  
        Console.WriteLine("...No recognition result.");  
      }  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    // Handle events.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">O reconhecedor não tem nenhuma gramática de reconhecimento de fala carregada.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> é <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> é a cadeia de caracteres vazia ("").</exception>
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Uma matriz de unidades do word que contém a entrada para a operação de reconhecimento.</param>
        <param name="compareOptions">Uma combinação bit a bit dos valores de enumeração que descreve o tipo de comparação a ser usado para a operação de reconhecimento emulado.</param>
        <summary>Emula a entrada de palavras específicas para o reconhecedor de fala, usando texto em vez de áudio para reconhecimento de fala síncrona e especifica como o reconhecedor trata Unicode comparação entre as palavras e as gramáticas de reconhecimento de fala carregado.</summary>
        <returns>O resultado da operação de reconhecimento, ou <see langword="null" /> se a operação não for bem-sucedida ou o reconhecedor não está habilitado.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 A gera do reconhecedor de fala a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como se a operação de reconhecimento não é emulada.  
  
 O reconhecedor usa `compareOptions` quando ele aplica regras da gramática a frase de entrada. Os identificadores que acompanham o Vista e Windows 7 Ignorar maiusculas e minúsculas, se o <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> ou <xref:System.Globalization.CompareOptions.IgnoreCase> valor está presente. O reconhecedor sempre ignora a largura de caractere e nunca ignora o tipo Kana. O reconhecedor também ignora novas linhas e espaço em branco extra e trata pontuação como entrada literal. Para obter mais informações sobre o tipo Kana e de largura de caractere, consulte o <xref:System.Globalization.CompareOptions> enumeração.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">O reconhecedor não tem nenhuma gramática de reconhecimento de fala carregada.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> é <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" />contém um ou mais <see langword="null" /> elementos.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" />contém o <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />, ou <see cref="F:System.Globalization.CompareOptions.StringSort" /> sinalizador.</exception>
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult EmulateRecognize (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult EmulateRecognize(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize(System.String,System.Globalization.CompareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">A frase de entrada para a operação de reconhecimento.</param>
        <param name="compareOptions">Uma combinação bit a bit dos valores de enumeração que descreve o tipo de comparação a ser usado para a operação de reconhecimento emulado.</param>
        <summary>Emula a entrada de uma frase para o reconhecedor de fala, usando texto em vez de áudio para reconhecimento de fala síncrona e especifica como o reconhecedor trata a comparação Unicode entre a frase e as gramáticas de reconhecimento de fala carregado.</summary>
        <returns>O resultado da operação de reconhecimento, ou <see langword="null" /> se a operação não for bem-sucedida ou o reconhecedor não está habilitado.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 A gera do reconhecedor de fala a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como se a operação de reconhecimento não é emulada.  
  
 O reconhecedor usa `compareOptions` quando ele aplica regras da gramática a frase de entrada. Os identificadores que acompanham o Vista e Windows 7 Ignorar maiusculas e minúsculas, se o <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> ou <xref:System.Globalization.CompareOptions.IgnoreCase> valor está presente. O reconhecedor sempre ignora a largura de caractere e nunca ignora o tipo Kana. O reconhecedor também ignora novas linhas e espaço em branco extra e trata pontuação como entrada literal. Para obter mais informações sobre o tipo Kana e de largura de caractere, consulte o <xref:System.Globalization.CompareOptions> enumeração.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">O reconhecedor não tem nenhuma gramática de reconhecimento de fala carregada.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> é <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> é a cadeia de caracteres vazia ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" />contém o <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />, ou <see cref="F:System.Globalization.CompareOptions.StringSort" /> sinalizador.</exception>
      </Docs>
    </Member>
    <MemberGroup MemberName="EmulateRecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Emula a entrada para o reconhecedor de fala, usando texto em vez de áudio para o reconhecimento de fala assíncrona.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Esses métodos ignorar a entrada de áudio do sistema e fornecer o texto para o reconhecedor como <xref:System.String> objetos ou como uma matriz de <xref:System.Speech.Recognition.RecognizedWordUnit> objetos. Isso pode ser útil quando você estiver testando ou depurar um aplicativo ou gramática. Por exemplo, você pode usar emulação para determinar se uma palavra está em uma gramática e quais semântica é retornada quando a palavra é reconhecida. Use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull%2A> método para desabilitar a entrada de áudio para o mecanismo de reconhecimento de voz durante as operações de emulação.  
  
 A gera do reconhecedor de fala a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como se a operação de reconhecimento não é emulada. Quando o reconhecedor conclui a operação de reconhecimento assíncrona, ela gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento. O reconhecedor ignora novas linhas e espaço em branco extra e trata pontuação como entrada literal.  
  
> [!NOTE]
>  O <xref:System.Speech.Recognition.RecognitionResult> objeto gerado pelo reconhecedor de fala em resposta à entrada emulada tem um valor de `null` para sua <xref:System.Speech.Recognition.RecognitionResult.Audio%2A> propriedade.  
  
 Para emular reconhecimento síncrono, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> método.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="inputText">A entrada para a operação de reconhecimento.</param>
        <summary>Emula a entrada de uma frase para o reconhecedor de fala, usando texto em vez de áudio para o reconhecimento de fala assíncrona.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 A gera do reconhecedor de fala a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como se a operação de reconhecimento não é emulada. Quando o reconhecedor conclui a operação de reconhecimento assíncrona, ela gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento.  
  
 Os identificadores que acompanham o Vista e Windows 7 Ignorar maiusculas e minúsculas e largura de caracteres ao aplicar regras de gramática para a expressão de entrada. Para obter mais informações sobre esse tipo de comparação, consulte o <xref:System.Globalization.CompareOptions> valores de enumeração <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> e <xref:System.Globalization.CompareOptions.IgnoreWidth>. Os identificadores também ignoram linhas novas e espaço em branco extra e tratam a pontuação como entrada literal.  
  
   
  
## Examples  
 O exemplo de código a seguir é parte de um aplicativo de console que demonstra a entrada de emulado assíncrona, os resultados do reconhecimento associados e os eventos associados emitidos o reconhecedor de fala. O exemplo gera a saída a seguir.  
  
```  
  
TestRecognizeAsync("Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = Smith  
 Done.  
  
TestRecognizeAsync("Jones")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Jones; Text = Jones  
 EmulateRecognizeCompleted event raised.  
  Grammar = Jones; Text = Jones  
 Done.  
  
TestRecognizeAsync("Mister")...  
 SpeechDetected event raised.  
 SpeechHypothesized event raised.  
  Grammar = Smith; Text = mister  
 SpeechRecognitionRejected event raised.  
  Grammar = <not available>; Text =  
 EmulateRecognizeCompleted event raised.  
  No recognition result available.  
 Done.  
  
TestRecognizeAsync("Mister Smith")...  
 SpeechDetected event raised.  
 SpeechRecognized event raised.  
  Grammar = Smith; Text = mister Smith  
 EmulateRecognizeCompleted event raised.  
  Grammar = Smith; Text = mister Smith  
 Done.  
  
press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace SreEmulateRecognizeAsync  
{  
  class Program  
  {  
    // Indicate when an asynchronous operation is finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Load grammars.  
        recognizer.LoadGrammar(CreateNameGrammar("Smith"));  
        recognizer.LoadGrammar(CreateNameGrammar("Jones"));  
  
        // Configure the audio input.  
        recognizer.SetInputToNull();  
  
        // Add event handlers for the events raised by the  
        // EmulateRecognizeAsync method.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHander);  
  
        // Start four asynchronous emulated recognition operations.  
        TestRecognizeAsync(recognizer, "Smith");  
        TestRecognizeAsync(recognizer, "Jones");  
        TestRecognizeAsync(recognizer, "Mister");  
        TestRecognizeAsync(recognizer, "Mister Smith");  
      }  
  
      Console.WriteLine("press any key to exit...");  
      Console.ReadKey(true);  
    }  
  
    // Create a simple name grammar.  
    // Set the grammar name to the surname.  
    private static Grammar CreateNameGrammar(string surname)  
    {  
      GrammarBuilder builder = new GrammarBuilder("mister", 0, 1);  
      builder.Append(surname);  
  
      Grammar nameGrammar = new Grammar(builder);  
      nameGrammar.Name = surname;  
  
      return nameGrammar;  
    }  
  
    // Send emulated input to the recognizer for asynchronous  
    // recognition.  
    private static void TestRecognizeAsync(  
      SpeechRecognitionEngine recognizer, string input)  
    {  
      completed = false;  
  
      Console.WriteLine("TestRecognizeAsync(\"{0}\")...", input);  
      recognizer.EmulateRecognizeAsync(input);  
  
      // Wait for the operation to complete.  
      while (!completed)  
      {  
        Thread.Sleep(333);  
      }  
  
      Console.WriteLine(" Done.");  
      Console.WriteLine();  
    }  
  
    static void SpeechDetectedHandler(  
      object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechDetected event raised.");  
    }  
  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechHypothesized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    // Handle events.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognitionRejected event raised.");  
      if (e.Result != null)  
      {  
        string grammarName;  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name ?? "<none>";  
        }  
        else  
        {  
          grammarName = "<not available>";  
        }  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          grammarName, e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" SpeechRecognized event raised.");  
      if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text );  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
    }  
  
    static void EmulateRecognizeCompletedHander(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" EmulateRecognizeCompleted event raised.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("  {0} exception encountered: {1}:",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      else if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      else if (e.Result != null)  
      {  
        Console.WriteLine("  Grammar = {0}; Text = {1}",  
          e.Result.Grammar.Name ?? "<none>", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  No recognition result available.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">O reconhecedor não tem nenhuma gramática de reconhecimento de fala carregada ou o reconhecedor tem uma operação assíncrona reconhecimento que ainda não está completa.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> é <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> é a cadeia de caracteres vazia ("").</exception>
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (System.Speech.Recognition.RecognizedWordUnit[] wordUnits, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(class System.Speech.Recognition.RecognizedWordUnit[] wordUnits, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.Speech.Recognition.RecognizedWordUnit[],System.Globalization.CompareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="wordUnits" Type="System.Speech.Recognition.RecognizedWordUnit[]" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="wordUnits">Uma matriz de unidades do word que contém a entrada para a operação de reconhecimento.</param>
        <param name="compareOptions">Uma combinação bit a bit dos valores de enumeração que descreve o tipo de comparação a ser usado para a operação de reconhecimento emulado.</param>
        <summary>Emula a entrada de palavras específicas para o reconhecedor de fala, usando uma matriz de <see cref="T:System.Speech.Recognition.RecognizedWordUnit" /> objetos no lugar de áudio para o reconhecimento de fala assíncrona e especifica como o reconhecedor trata Unicode comparação entre as palavras e as gramáticas de reconhecimento de fala carregado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 A gera do reconhecedor de fala a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como se a operação de reconhecimento não é emulada. Quando o reconhecedor conclui a operação de reconhecimento assíncrona, ela gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento.  
  
 O reconhecedor usa `compareOptions` quando ele aplica regras da gramática a frase de entrada. Os identificadores que acompanham o Vista e Windows 7 Ignorar maiusculas e minúsculas, se o <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> ou <xref:System.Globalization.CompareOptions.IgnoreCase> valor está presente. Os identificadores sempre ignorar a largura de caractere e nunca ignorar o tipo Kana. Os identificadores também ignoram linhas novas e espaço em branco extra e tratam a pontuação como entrada literal. Para obter mais informações sobre o tipo Kana e de largura de caractere, consulte o <xref:System.Globalization.CompareOptions> enumeração.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">O reconhecedor não tem nenhuma gramática de reconhecimento de fala carregada ou o reconhecedor tem uma operação assíncrona reconhecimento que ainda não está completa.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="wordUnits" /> é <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="wordUnits" />contém um ou mais <see langword="null" /> elementos.</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" />contém o <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />, ou <see cref="F:System.Globalization.CompareOptions.StringSort" /> sinalizador.</exception>
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeAsync">
      <MemberSignature Language="C#" Value="public void EmulateRecognizeAsync (string inputText, System.Globalization.CompareOptions compareOptions);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void EmulateRecognizeAsync(string inputText, valuetype System.Globalization.CompareOptions compareOptions) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync(System.String,System.Globalization.CompareOptions)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="inputText" Type="System.String" />
        <Parameter Name="compareOptions" Type="System.Globalization.CompareOptions" />
      </Parameters>
      <Docs>
        <param name="inputText">A frase de entrada para a operação de reconhecimento.</param>
        <param name="compareOptions">Uma combinação bit a bit dos valores de enumeração que descreve o tipo de comparação a ser usado para a operação de reconhecimento emulado.</param>
        <summary>Emula a entrada de uma frase para o reconhecedor de fala, usando texto em vez de áudio para reconhecimento de fala assíncrona e especifica como o reconhecedor trata a comparação Unicode entre a frase e as gramáticas de reconhecimento de fala carregado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 A gera do reconhecedor de fala a <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos como se a operação de reconhecimento não é emulada. Quando o reconhecedor conclui a operação de reconhecimento assíncrona, ela gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento.  
  
 O reconhecedor usa `compareOptions` quando ele aplica regras da gramática a frase de entrada. Os identificadores que acompanham o Vista e Windows 7 Ignorar maiusculas e minúsculas, se o <xref:System.Globalization.CompareOptions.OrdinalIgnoreCase> ou <xref:System.Globalization.CompareOptions.IgnoreCase> valor está presente. Os identificadores sempre ignorar a largura de caractere e nunca ignorar o tipo Kana. Os identificadores também ignoram linhas novas e espaço em branco extra e tratam a pontuação como entrada literal. Para obter mais informações sobre o tipo Kana e de largura de caractere, consulte o <xref:System.Globalization.CompareOptions> enumeração.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.InvalidOperationException">O reconhecedor não tem nenhuma gramática de reconhecimento de fala carregada ou o reconhecedor tem uma operação assíncrona reconhecimento que ainda não está completa.</exception>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="inputText" /> é <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="inputText" /> é a cadeia de caracteres vazia ("").</exception>
        <exception cref="T:System.NotSupportedException">
          <paramref name="compareOptions" />contém o <see cref="F:System.Globalization.CompareOptions.IgnoreNonSpace" />, <see cref="F:System.Globalization.CompareOptions.IgnoreSymbols" />, ou <see cref="F:System.Globalization.CompareOptions.StringSort" /> sinalizador.</exception>
      </Docs>
    </Member>
    <Member MemberName="EmulateRecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt; EmulateRecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.EmulateRecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Gerado quando o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> Finaliza uma operação assíncrona de reconhecimento de entrada emulada.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cada <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> método começa uma operação assíncrona de reconhecimento. O <xref:System.Speech.Recognition.SpeechRecognitionEngine> gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento quando ele finaliza a operação assíncrona.  
  
 O <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> operação pode gerar o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos. O <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento é o último evento desse tipo que o reconhecedor gera para uma determinada operação.  
  
 Se o reconhecimento emulado foi bem-sucedida, você pode acessar o resultado de reconhecimento usando um dos seguintes:  
  
-   O <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> propriedade o <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> objeto no manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> evento.  
  
-   <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A>propriedade no <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> objeto no manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento.  
  
 Se o reconhecimento emulado não foi bem-sucedida, o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> não é gerado e o <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs.Result%2A> será nulo.  
  
 <xref:System.Speech.Recognition.EmulateRecognizeCompletedEventArgs> deriva de <xref:System.ComponentModel.AsyncCompletedEventArgs>.  
  
 <xref:System.Speech.Recognition.SpeechRecognizedEventArgs> deriva de <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Quando você cria um <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> delegado, você identificar o método que manipulará o evento. Para associar o evento com o manipulador de eventos, adicione uma instância do representante ao evento. O manipulador de eventos é chamado sempre que o evento ocorre, a menos que você remova o representante. Para obter mais informações sobre delegados de manipulador de eventos, consulte [eventos e delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 O exemplo a seguir é parte de um aplicativo de console que carrega uma gramática de reconhecimento de fala e demonstra a entrada de emulado assíncrona, os resultados do reconhecimento associados e os eventos associados emitidos o reconhecedor de fala.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InProcessRecognizer  
{  
  class Program  
  {  
    // Indicate whether the asynchronous emulate recognition  
    // operation has completed.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
  
      // Initialize an instance of an in-process recognizer.  
      using (SpeechRecognitionEngine recognizer =   
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a sample grammar.  
        Grammar testGrammar =  
          new Grammar(new GrammarBuilder("testing testing"));  
        testGrammar.Name = "Test Grammar";  
        recognizer.LoadGrammar(testGrammar);  
  
        // Attach event handlers for recognition events.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(SpeechRecognizedHandler);  
        recognizer.EmulateRecognizeCompleted +=  
          new EventHandler<EmulateRecognizeCompletedEventArgs>(  
            EmulateRecognizeCompletedHandler);  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call mathches the grammar  
        // and generates a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing testing");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        completed = false;  
  
        // This EmulateRecognizeAsync call does not match the grammar  
        // or generate a SpeechRecognized event.  
        recognizer.EmulateRecognizeAsync("testing one two three");  
  
        // Wait for the asynchronous operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null)  
      {  
        Console.WriteLine("Result of 1st call to EmulateRecognizeAsync = {0}",  
          e.Result.Text ?? "<no text>");  
        Console.WriteLine();  
      }  
      else  
      {  
        Console.WriteLine("No recognition result");  
      }  
    }  
  
    // Handle the EmulateRecognizeCompleted event.  
    static void EmulateRecognizeCompletedHandler(  
      object sender, EmulateRecognizeCompletedEventArgs e)  
    {  
      if (e.Result == null)  
      {  
        Console.WriteLine("Result of 2nd call to EmulateRecognizeAsync = No result generated.");  
      }  
  
      // Indicate the asynchronous operation is complete.  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtém ou define o intervalo de silêncio que o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> aceitará ao final de uma entrada não ambígua antes de finalizar uma operação de reconhecimento.</summary>
        <value>A duração do intervalo de silêncio.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O reconhecedor de fala usa esse intervalo de tempo limite quando a entrada de reconhecimento é ambígua. Por exemplo, para uma gramática de reconhecimento de fala que oferece suporte ao reconhecimento do "novo jogo." ou "novo jogo", "novo jogo," é uma entrada ambígua, e "novo jogo" é uma entrada ambígua.  
  
 Essa propriedade determina quanto tempo o reconhecimento de fala aguardará entrada adicional antes de concluir uma operação de reconhecimento. O intervalo de tempo limite pode ser de 0 segundos para 10 segundos, inclusivos. O padrão é de 150 milissegundos.  
  
 Para definir o intervalo de tempo limite para a entrada ambíguo, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriedade.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Essa propriedade é definida como menor que 0 segundo ou maior que 10 segundos.</exception>
      </Docs>
    </Member>
    <Member MemberName="EndSilenceTimeoutAmbiguous">
      <MemberSignature Language="C#" Value="public TimeSpan EndSilenceTimeoutAmbiguous { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan EndSilenceTimeoutAmbiguous" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtém ou define o intervalo de silêncio que o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> aceitará ao final de uma entrada ambígua antes de finalizar uma operação de reconhecimento.</summary>
        <value>A duração do intervalo de silêncio.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O reconhecedor de fala usa esse intervalo de tempo limite quando a entrada de reconhecimento é ambígua. Por exemplo, para uma gramática de reconhecimento de fala que oferece suporte ao reconhecimento do "novo jogo." ou "novo jogo", "novo jogo," é uma entrada ambígua, e "novo jogo" é uma entrada ambígua.  
  
 Essa propriedade determina quanto tempo o reconhecimento de fala aguardará entrada adicional antes de concluir uma operação de reconhecimento. O intervalo de tempo limite pode ser de 0 segundos para 10 segundos, inclusivos. O padrão é 500 milissegundos.  
  
 Para definir o intervalo de tempo limite para a entrada não ambígua, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> propriedade.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Essa propriedade é definida como menor que 0 segundo ou maior que 10 segundos.</exception>
      </Docs>
    </Member>
    <Member MemberName="Grammars">
      <MemberSignature Language="C#" Value="public System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt; Grammars { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.Grammar&gt; Grammars" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.Grammars" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.Grammar&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtém uma coleção do <see cref="T:System.Speech.Recognition.Grammar" /> objetos que são carregados na <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> instância.</summary>
        <value>A coleção de objetos <see cref="T:System.Speech.Recognition.Grammar" />.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 O exemplo a seguir envia informações para o console para cada gramática de reconhecimento de fala está carregado no momento por um reconhecedor de fala.  
  
> [!IMPORTANT]
>  Copie a coleção de gramática para evitar erros se a coleção é modificada enquanto esse método enumera os elementos da coleção.  
  
```csharp  
  
private static void ListGrammars(SpeechRecognitionEngine recognizer)  
{  
  string qualifier;  
  List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
  foreach (Grammar g in grammars)  
  {  
    qualifier = (g.Enabled) ? "enabled" : "disabled";  
  
    Console.WriteLine("Grammar {0} is loaded and is {1}.",  
      g.Name, qualifier);  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="InitialSilenceTimeout">
      <MemberSignature Language="C#" Value="public TimeSpan InitialSilenceTimeout { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan InitialSilenceTimeout" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Attributes>
        <Attribute>
          <AttributeName>System.ComponentModel.EditorBrowsable(System.ComponentModel.EditorBrowsableState.Advanced)</AttributeName>
        </Attribute>
      </Attributes>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtém ou define o intervalo de tempo durante o qual um <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> aceita entradas contendo apenas silêncio antes de finalizar o reconhecimento.</summary>
        <value>A duração do intervalo de silêncio.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cada reconhecedor de fala tem um algoritmo para distinguir entre silêncio e fala. Se a entrada do reconhecedor for silêncio durante o período de tempo limite de silêncio inicial, o reconhecedor finaliza a operação reconhecimento.  
  
-   Para operações assíncronas de reconhecimento e emulação, gera o reconhecedor de <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento, onde o <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A?displayProperty=nameWithType> é de propriedade `true`e o <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A?displayProperty=nameWithType> é de propriedade `null`.  
  
-   Para operações síncronas reconhecimento e emulação, retorna o reconhecedor `null`, em vez de uma opção válida <xref:System.Speech.Recognition.RecognitionResult>.  
  
 Se o intervalo de tempo limite de silêncio inicial é definido como 0, o reconhecedor não executa uma verificação de tempo limite de silêncio inicial. O intervalo de tempo limite pode ser qualquer valor negativo. O padrão é 0 segundos.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala básico. O exemplo define o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriedades de um <xref:System.Speech.Recognition.SpeechRecognitionEngine> antes de iniciar o reconhecimento de fala. Manipuladores para o reconhecedor de fala <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioStateChanged> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos de saída de informações de evento para o console para demonstrar como o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriedades de um <xref:System.Speech.Recognition.SpeechRecognitionEngine> propriedades afetam as operações de reconhecimento.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Load a Grammar object.  
        recognizer.LoadGrammar(CreateServicesGrammar("FindServices"));  
  
        // Add event handlers.  
        recognizer.AudioStateChanged +=  
          new EventHandler<AudioStateChangedEventArgs>(  
            AudioStateChangedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(3);  
        recognizer.BabbleTimeout = TimeSpan.FromSeconds(2);  
        recognizer.EndSilenceTimeout = TimeSpan.FromSeconds(1);  
        recognizer.EndSilenceTimeoutAmbiguous = TimeSpan.FromSeconds(1.5);  
  
        Console.WriteLine("BabbleTimeout: {0}", recognizer.BabbleTimeout);  
        Console.WriteLine("InitialSilenceTimeout: {0}", recognizer.InitialSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeout: {0}", recognizer.EndSilenceTimeout);  
        Console.WriteLine("EndSilenceTimeoutAmbiguous: {0}", recognizer.EndSilenceTimeoutAmbiguous);  
        Console.WriteLine();  
  
        // Start asynchronous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Single);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Create a grammar and build it into a Grammar object.   
    static Grammar CreateServicesGrammar(string grammarName)  
    {  
  
      // Create a grammar for finding services in different cities.  
      Choices services = new Choices(new string[] { "restaurants", "hotels", "gas stations" });  
      Choices cities = new Choices(new string[] { "Seattle", "Boston", "Dallas" });  
  
      GrammarBuilder findServices = new GrammarBuilder("Find");  
      findServices.Append(services);  
      findServices.Append("near");  
      findServices.Append(cities);  
  
      // Create a Grammar object from the GrammarBuilder..  
      Grammar servicesGrammar = new Grammar(findServices);  
      servicesGrammar.Name = ("FindServices");  
      return servicesGrammar;  
    }  
  
    // Handle the AudioStateChanged event.  
    static void AudioStateChangedHandler(  
      object sender, AudioStateChangedEventArgs e)  
    {  
      Console.WriteLine("AudioStateChanged ({0}): {1}",  
        DateTime.Now.ToString("mm:ss.f"), e.AudioState);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("RecognizeCompleted ({0}):",  
        DateTime.Now.ToString("mm:ss.f"));  
  
      string resultText;  
      if (e.Result != null) { resultText = e.Result.Text; }  
      else { resultText = "<null>"; }  
  
      Console.WriteLine(  
        " BabbleTimeout: {0}; InitialSilenceTimeout: {1}; Result text: {2}",  
        e.BabbleTimeout, e.InitialSilenceTimeout, resultText);  
      if (e.Error != null)  
      {  
        Console.WriteLine(" Exception message: ", e.Error.Message);  
      }  
  
      // Start the next asynchronous recognition operation.  
      ((SpeechRecognitionEngine)sender).RecognizeAsync(RecognizeMode.Single);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">Esta propriedade está definida como menos de 0 segundos.</exception>
      </Docs>
    </Member>
    <Member MemberName="InstalledRecognizers">
      <MemberSignature Language="C#" Value="public static System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers ();" />
      <MemberSignature Language="ILAsm" Value=".method public static hidebysig class System.Collections.ObjectModel.ReadOnlyCollection`1&lt;class System.Speech.Recognition.RecognizerInfo&gt; InstalledRecognizers() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Collections.ObjectModel.ReadOnlyCollection&lt;System.Speech.Recognition.RecognizerInfo&gt;</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Retorna as informações de todos os identificadores de fala instalados no sistema atual.</summary>
        <returns>Uma coleção somente leitura dos objetos <see cref="T:System.Speech.Recognition.RecognizerInfo" /> que descrevem os reconhecedores instalados.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Para obter informações sobre o identificador atual, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo%2A> propriedade.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala básico. O exemplo usa a coleção retornada pelo <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> método para encontrar um reconhecedor de fala que dá suporte ao idioma inglês.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Select a speech recognizer that supports English.  
      RecognizerInfo info = null;  
      foreach (RecognizerInfo ri in SpeechRecognitionEngine.InstalledRecognizers())  
      {  
        if (ri.Culture.TwoLetterISOLanguageName.Equals("en"))  
        {  
          info = ri;  
          break;  
        }  
      }  
      if (info == null) return;  
  
      // Create the selected recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(info))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="LoadGrammar">
      <MemberSignature Language="C#" Value="public void LoadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">O objeto de gramática a ser carregado.</param>
        <summary>Carrega de forma síncrona um objeto <see cref="T:System.Speech.Recognition.Grammar" />.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O reconhecedor lança uma exceção se o <xref:System.Speech.Recognition.Grammar> objeto já está carregado, está sendo carregado de forma assíncrona ou falhou ao carregar qualquer reconhecedor. Não é possível carregar o mesmo <xref:System.Speech.Recognition.Grammar> objeto em várias instâncias do <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Em vez disso, crie um novo <xref:System.Speech.Recognition.Grammar> objeto para cada <xref:System.Speech.Recognition.SpeechRecognitionEngine> instância.  
  
 Se o reconhecedor está em execução, os aplicativos devem usar <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> para pausar o mecanismo de reconhecimento de fala antes do carregamento, descarregamento, habilitar ou desabilitar uma gramática.  
  
 Quando você carrega uma gramática, ele é habilitado por padrão. Para desabilitar uma gramática carregada, use o <xref:System.Speech.Recognition.Grammar.Enabled%2A> propriedade.  
  
 Para carregar um <xref:System.Speech.Recognition.Grammar> objeto de forma assíncrona, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala básico. O exemplo cria um <xref:System.Speech.Recognition.DictationGrammar> e os carrega em um reconhecedor de fala.  
  
```csharp  
using System;  
using System.Speech.Recognition;  
  
namespace SpeechRecognitionApp  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
  
      // Create an in-process speech recognizer for the en-US locale.  
      using (  
      SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Add a handler for the speech recognized event.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous speech recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        while (true)  
        {  
          Console.ReadLine();  
        }  
      }  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Recognized text: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> é <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> não está em um objeto válido.</exception>
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarAsync">
      <MemberSignature Language="C#" Value="public void LoadGrammarAsync (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void LoadGrammarAsync(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync(System.Speech.Recognition.Grammar)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">A gramática de reconhecimento de fala a ser carregada.</param>
        <summary>Carrega de forma assíncrona uma gramática de reconhecimento de fala.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Quando o reconhecedor de completa o carregamento um <xref:System.Speech.Recognition.Grammar> do objeto, ele gera um <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> eventos. O reconhecedor lança uma exceção se o <xref:System.Speech.Recognition.Grammar> objeto já está carregado, está sendo carregado de forma assíncrona ou falhou ao carregar qualquer reconhecedor. Não é possível carregar o mesmo <xref:System.Speech.Recognition.Grammar> objeto em várias instâncias do <xref:System.Speech.Recognition.SpeechRecognitionEngine>. Em vez disso, crie um novo <xref:System.Speech.Recognition.Grammar> objeto para cada <xref:System.Speech.Recognition.SpeechRecognitionEngine> instância.  
  
 Se o reconhecedor está em execução, os aplicativos devem usar <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> para pausar o mecanismo de reconhecimento de fala antes do carregamento, descarregamento, habilitar ou desabilitar uma gramática.  
  
 Quando você carrega uma gramática, ele é habilitado por padrão. Para desabilitar uma gramática carregada, use o <xref:System.Speech.Recognition.Grammar.Enabled%2A> propriedade.  
  
 Para carregar uma gramática de reconhecimento de fala de forma síncrona, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> método.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> é <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">
          <paramref name="Grammar" /> não está em um objeto válido.</exception>
        <exception cref="T:System.OperationCanceledException">A operação assíncrona foi cancelada.</exception>
      </Docs>
    </Member>
    <Member MemberName="LoadGrammarCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt; LoadGrammarCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.LoadGrammarCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Gerado quando o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> termina o carregamento assíncrono de um <see cref="T:System.Speech.Recognition.Grammar" /> objeto.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método inicia uma operação assíncrona. O <xref:System.Speech.Recognition.SpeechRecognitionEngine> gera esse evento quando concluir a operação. Para obter o <xref:System.Speech.Recognition.Grammar> do objeto que o reconhecedor carregado, use o <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs.Grammar%2A> propriedade associado <xref:System.Speech.Recognition.LoadGrammarCompletedEventArgs>. Para obter atual <xref:System.Speech.Recognition.Grammar> o reconhecedor tiver sido carregado, os objetos usam o reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.Grammars%2A> propriedade.  
  
 Se o reconhecedor está em execução, os aplicativos devem usar <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> para pausar o mecanismo de reconhecimento de fala antes do carregamento, descarregamento, habilitar ou desabilitar uma gramática.  
  
 Quando você cria um <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> delegado, você identificar o método que manipulará o evento. Para associar o evento com o manipulador de eventos, adicione uma instância do representante ao evento. O manipulador de eventos é chamado sempre que o evento ocorre, a menos que você remova o representante. Para obter mais informações sobre delegados de manipulador de eventos, consulte [eventos e delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 O exemplo a seguir cria um reconhecedor de fala em processo e, em seguida, cria dois tipos de gramáticas para reconhecer palavras específicas e aceitando ditado livre. O exemplo constrói uma <xref:System.Speech.Recognition.Grammar> objeto de cada as gramáticas de reconhecimento de fala concluído assincronamente carrega o <xref:System.Speech.Recognition.Grammar> objetos para o <xref:System.Speech.Recognition.SpeechRecognitionEngine> instância. Manipuladores para o reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarCompleted> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos gravar o nome no console do <xref:System.Speech.Recognition.Grammar> objeto que foi usado para realizar o reconhecimento e o texto do resultado reconhecimento, respectivamente.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and set its input.  
      recognizer = new SpeechRecognitionEngine();  
      recognizer.SetInputToDefaultAudioDevice();  
  
      // Add a handler for the LoadGrammarCompleted event.  
      recognizer.LoadGrammarCompleted +=  
        new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
      // Add a handler for the SpeechRecognized event.  
      recognizer.SpeechRecognized +=  
        new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
      // Create the "yesno" grammar.  
      Choices yesChoices = new Choices(new string[] { "yes", "yup", "yeah" });  
      SemanticResultValue yesValue =  
          new SemanticResultValue(yesChoices, (bool)true);  
      Choices noChoices = new Choices(new string[] { "no", "nope", "neah" });  
      SemanticResultValue noValue =  
          new SemanticResultValue(noChoices, (bool)false);  
      SemanticResultKey yesNoKey =  
          new SemanticResultKey("yesno", new Choices(new GrammarBuilder[] { yesValue, noValue }));  
      Grammar yesnoGrammar = new Grammar(yesNoKey);  
      yesnoGrammar.Name = "yesNo";  
  
      // Create the "done" grammar.  
      Grammar doneGrammar =  
        new Grammar(new Choices(new string[] { "done", "exit", "quit", "stop" }));  
      doneGrammar.Name = "Done";  
  
      // Create a dictation grammar.  
      Grammar dictation = new DictationGrammar();  
      dictation.Name = "Dictation";  
  
      // Load grammars to the recognizer.  
      recognizer.LoadGrammarAsync(yesnoGrammar);  
      recognizer.LoadGrammarAsync(doneGrammar);  
      recognizer.LoadGrammarAsync(dictation);  
  
      // Start asynchronous, continuous recognition.  
      recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
      // Keep the console window open.  
      Console.ReadLine();  
    }  
  
    // Handle the LoadGrammarCompleted event.   
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      string grammarName = e.Grammar.Name;  
      bool grammarLoaded = e.Grammar.Loaded;  
  
      if (e.Error != null)  
      {  
        Console.WriteLine("LoadGrammar for {0} failed with a {1}.",  
        grammarName, e.Error.GetType().Name);  
  
        // Add exception handling code here.  
      }  
  
      Console.WriteLine("Grammar {0} {1} loaded.",  
      grammarName, (grammarLoaded) ? "is" : "is not");  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Grammar({0}): {1}", e.Result.Grammar.Name, e.Result.Text);  
  
      // Add event handler code here.  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="MaxAlternates">
      <MemberSignature Language="C#" Value="public int MaxAlternates { get; set; }" />
      <MemberSignature Language="ILAsm" Value=".property instance int32 MaxAlternates" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Int32</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtém ou define o número máximo de resultados do reconhecimento alternativo que o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> retorna para cada operação de reconhecimento.</summary>
        <value>O número de resultados alternativos a ser retornada.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> propriedade o <xref:System.Speech.Recognition.RecognitionResult> classe contém a coleção de <xref:System.Speech.Recognition.RecognizedPhrase> objetos que representam possíveis interpretações de entrada.  
  
 O valor padrão para <xref:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates%2A> é 10.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentOutOfRangeException">
          <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.MaxAlternates" />é definido como um valor menor que 0.</exception>
      </Docs>
    </Member>
    <Member MemberName="QueryRecognizerSetting">
      <MemberSignature Language="C#" Value="public object QueryRecognizerSetting (string settingName);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance object QueryRecognizerSetting(string settingName) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting(System.String)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Object</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">O nome da configuração a retornar.</param>
        <summary>Retorna os valores das configurações do reconhecedor.</summary>
        <returns>O valor da configuração.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Configurações do reconhecedor podem conter a cadeia de caracteres, inteiro de 64 bits ou dados de endereço de memória. A tabela a seguir descreve as configurações que são definidas para uma API do Microsoft Speech (SAPI)-reconhecedor compatível. As configurações a seguir devem ter o mesmo intervalo para cada identificador que suporta a configuração. Um identificador compatível com SAPI não é necessário para dar suporte a essas configurações e pode dar suporte a outras configurações.  
  
|Nome|Descrição|  
|----------|-----------------|  
|`ResourceUsage`|Especifica o consumo de CPU do reconhecedor. O intervalo é de 0 a 100. O valor padrão é 50.|  
|`ResponseSpeed`|Indica o comprimento de silêncio no final da entrada ambíguo antes do reconhecedor de fala conclui uma operação de reconhecimento. O intervalo é de 0 a 10.000 milissegundos (ms). Essa configuração corresponde do reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> propriedade.  Padrão = 150 ms.|  
|`ComplexResponseSpeed`|Indica o comprimento de silêncio no final da entrada ambíguo antes do reconhecedor de fala conclui uma operação de reconhecimento. O intervalo é de 0 a 10.000 ms. Essa configuração corresponde do reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriedade. Padrão = 500 ms.|  
|`AdaptationOn`|Indica se a adaptação do modelo acústico está ON (valor = `1`) ou OFF (valor = `0`). O valor padrão é `1` (ON).|  
|`PersistedBackgroundAdaptation`|Indica se a adaptação de plano de fundo é ON (valor = `1`) ou OFF (valor = `0`), e persistir a configuração no registro. O valor padrão é `1` (ON).|  
  
 Para atualizar uma configuração do reconhecedor, use uma da <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> métodos.  
  
   
  
## Examples  
 O exemplo a seguir é parte de um aplicativo de console que gera os valores para um número das configurações definidas para o reconhecedor que oferece suporte a localidade en-US. O exemplo gera a saída a seguir.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation"  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        foreach (string setting in settings)  
        {  
          try  
          {  
            object value = recognizer.QueryRecognizerSetting(setting);  
            Console.WriteLine("  {0,-30} = {1}", setting, value);  
          }  
          catch  
          {  
            Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
              setting);  
          }  
        }  
      }  
      Console.WriteLine();  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> é <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> é a cadeia de caracteres vazia ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">O reconhecedor não tem uma configuração com esse nome.</exception>
      </Docs>
    </Member>
    <MemberGroup MemberName="Recognize">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Inicia uma operação de reconhecimento de fala síncrona.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Esses métodos de executam uma operação de reconhecimento de único, síncrono. O reconhecedor executa esta operação contra as gramáticas de reconhecimento de fala carregados e habilitados.  
  
 Durante uma chamada para esse método, o reconhecedor pode gerar os seguintes eventos:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Gerado quando o reconhecedor detecta a entrada, ele pode identificar como fala.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Gerado quando a entrada cria uma correspondência ambígua com uma das gramáticas ativas.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Gerado quando o reconhecedor Finaliza uma operação de reconhecimento.  
  
 O reconhecedor não gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos quando um do <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> métodos.  
  
 O <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> métodos retornam um <xref:System.Speech.Recognition.RecognitionResult> objeto, ou `null` se a operação não for bem-sucedida ou o reconhecedor não está habilitado.  
  
 Uma operação síncrona de reconhecimento pode falhar pelos seguintes motivos:  
  
-   Fala não for detectada antes de expirarem os intervalos de tempo limite para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriedades, ou para o `initialSilenceTimeout` parâmetro o <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> método.  
  
-   O mecanismo de reconhecimento detecta fala, mas não encontrar correspondências em qualquer um dos seus carregados e habilitados <xref:System.Speech.Recognition.Grammar> objetos.  
  
 Para modificar como o reconhecedor controla o tempo de fala ou silêncio em relação ao reconhecimento, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriedades.  
  
 O <xref:System.Speech.Recognition.SpeechRecognitionEngine> devem ter pelo menos um <xref:System.Speech.Recognition.Grammar> objeto carregado antes de realizar o reconhecimento. Para carregar uma gramática de reconhecimento de fala, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
 Para executar o reconhecimento assíncrono, use um do <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> métodos.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Executa uma operação de reconhecimento de fala síncrona.</summary>
        <returns>O resultado do reconhecimento para a entrada ou <see langword="null" /> se a operação não for bem-sucedida ou o reconhecedor não estiver habilitado.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Esse método executa uma operação única de reconhecimento. O reconhecedor executa esta operação contra as gramáticas de reconhecimento de fala carregados e habilitados.  
  
 Durante uma chamada para esse método, o reconhecedor pode gerar os seguintes eventos:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Gerado quando o reconhecedor detecta a entrada, ele pode identificar como fala.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Gerado quando a entrada cria uma correspondência ambígua com uma das gramáticas ativas.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Gerado quando o reconhecedor Finaliza uma operação de reconhecimento.  
  
 O reconhecedor não gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento ao usar esse método.  
  
 O <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> método retorna um <xref:System.Speech.Recognition.RecognitionResult> objeto, ou `null` se a operação não for bem-sucedida.  
  
 Uma operação síncrona de reconhecimento pode falhar pelos seguintes motivos:  
  
-   Não a fala é detectada antes de expirarem os intervalos de tempo limite para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriedades.  
  
-   O mecanismo de reconhecimento detecta fala, mas não encontrar correspondências em qualquer um dos seus carregados e habilitados <xref:System.Speech.Recognition.Grammar> objetos.  
  
 Para executar o reconhecimento assíncrono, use um do <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> métodos.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala básico. O exemplo cria um <xref:System.Speech.Recognition.DictationGrammar>, carrega-os em um reconhecedor de fala em processo e executa uma operação de reconhecimento.  
  
```  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Modify the initial silence time-out value.  
        recognizer.InitialSilenceTimeout = TimeSpan.FromSeconds(5);  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize();  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="Recognize">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognitionResult Recognize (TimeSpan initialSilenceTimeout);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class System.Speech.Recognition.RecognitionResult Recognize(valuetype System.TimeSpan initialSilenceTimeout) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.Recognize(System.TimeSpan)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognitionResult</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="initialSilenceTimeout" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="initialSilenceTimeout">O intervalo de tempo durante o qual um reconhecedor de fala aceita uma entrada contendo apenas silêncio antes de finalizar o reconhecimento.</param>
        <summary>Executa uma operação síncrona de reconhecimento de fala com um período de tempo limite de silêncio inicial especificado.</summary>
        <returns>O resultado do reconhecimento para a entrada ou <see langword="null" /> se a operação não for bem-sucedida ou o reconhecedor não estiver habilitado.</returns>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se o mecanismo de reconhecimento de fala detecta fala dentro do intervalo de tempo especificado por `initialSilenceTimeout` argumento, <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%28System.TimeSpan%29> executa uma operação única de reconhecimento e, em seguida, termina.  O `initialSilenceTimeout` parâmetro substitui o reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriedade.  
  
 Durante uma chamada para esse método, o reconhecedor pode gerar os seguintes eventos:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Gerado quando o reconhecedor detecta a entrada, ele pode identificar como fala.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Gerado quando a entrada cria uma correspondência ambígua com uma das gramáticas ativas.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Gerado quando o reconhecedor Finaliza uma operação de reconhecimento.  
  
 O reconhecedor não gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento ao usar esse método.  
  
 O <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize> método retorna um <xref:System.Speech.Recognition.RecognitionResult> objeto, ou `null` se a operação não for bem-sucedida.  
  
 Uma operação síncrona de reconhecimento pode falhar pelos seguintes motivos:  
  
-   Não a fala é detectada antes de expirarem os intervalos de tempo limite para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> ou para o `initialSilenceTimeout` parâmetro.  
  
-   O mecanismo de reconhecimento detecta fala, mas não encontrar correspondências em qualquer um dos seus carregados e habilitados <xref:System.Speech.Recognition.Grammar> objetos.  
  
 Para executar o reconhecimento assíncrono, use um do <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> métodos.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala básico. O exemplo cria um <xref:System.Speech.Recognition.DictationGrammar>, carrega-os em um reconhecedor de fala em processo e executa uma operação de reconhecimento.  
  
```csharp  
  
using System;  
using System.Speech.Recognition;  
  
namespace SynchronousRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer for the en-US locale.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(  
          new System.Globalization.CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        recognizer.LoadGrammar(new DictationGrammar());  
  
        // Configure input to the speech recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start synchronous speech recognition.  
        RecognitionResult result = recognizer.Recognize(TimeSpan.FromSeconds(5));  
  
        if (result != null)  
        {  
          Console.WriteLine("Recognized text = {0}", result.Text);  
        }  
        else  
        {  
          Console.WriteLine("No recognition result available.");  
        }  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to continue...");  
      Console.ReadKey();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="RecognizeAsync">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Inicia uma operação de reconhecimento de fala assíncrona.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Esses métodos executam única ou várias operações assíncronas de reconhecimento. O reconhecedor executa cada operação nas gramáticas de reconhecimento de fala carregados e habilitados.  
  
 Durante uma chamada para esse método, o reconhecedor pode gerar os seguintes eventos:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Gerado quando o reconhecedor detecta a entrada, ele pode identificar como fala.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Gerado quando a entrada cria uma correspondência ambígua com uma das gramáticas ativas.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Gerado quando o reconhecedor Finaliza uma operação de reconhecimento.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Gerado quando um <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> operação concluída.  
  
 Para recuperar o resultado de uma operação assíncrona de reconhecimento, anexar um manipulador de eventos para o reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos. O reconhecedor gera esse evento sempre que ele concluir com êxito uma operação de reconhecimento síncronas ou assíncronas. Se o reconhecimento não foi bem-sucedida, o <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> propriedade <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> objeto, que você pode acessar o manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos, será `null`.  
  
 Uma operação assíncrona de reconhecimento pode falhar pelos seguintes motivos:  
  
-   Não a fala é detectada antes de expirarem os intervalos de tempo limite para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriedades.  
  
-   O mecanismo de reconhecimento detecta fala, mas não encontrar correspondências em qualquer um dos seus carregados e habilitados <xref:System.Speech.Recognition.Grammar> objetos.  
  
-   O <xref:System.Speech.Recognition.SpeechRecognitionEngine> devem ter pelo menos um <xref:System.Speech.Recognition.Grammar> objeto carregado antes de realizar o reconhecimento. Para carregar uma gramática de reconhecimento de fala, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammar%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.LoadGrammarAsync%2A> método.  
  
-   Para modificar como o reconhecedor controla o tempo de fala ou silêncio em relação ao reconhecimento, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriedades.  
  
-   Para executar o reconhecimento síncrono, use um do <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> métodos.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Executa uma única operação de reconhecimento de fala síncrona.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Esse método executa uma operação de reconhecimento de único e assíncrona. O reconhecedor executa a operação em relação as gramáticas de reconhecimento de fala carregados e habilitados.  
  
 Durante uma chamada para esse método, o reconhecedor pode gerar os seguintes eventos:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Gerado quando o reconhecedor detecta a entrada, ele pode identificar como fala.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Gerado quando a entrada cria uma correspondência ambígua com uma das gramáticas ativas.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Gerado quando o reconhecedor Finaliza uma operação de reconhecimento.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Gerado quando um <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> operação concluída.  
  
 Para recuperar o resultado de uma operação assíncrona de reconhecimento, anexar um manipulador de eventos para o reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos. O reconhecedor gera esse evento sempre que ele concluir com êxito uma operação de reconhecimento síncronas ou assíncronas. Se o reconhecimento não foi bem-sucedida, o <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> propriedade <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> objeto, que você pode acessar o manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos, será `null`.  
  
 Para executar o reconhecimento síncrono, use um do <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> métodos.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala assíncrona básico. O exemplo cria um <xref:System.Speech.Recognition.DictationGrammar>, carrega-os em um reconhecedor de fala em processo e executa uma operação assíncrona de reconhecimento. Manipuladores de eventos estão incluídos para demonstrar os eventos que o reconhecedor gera durante a operação.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[]   
        { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start an asynchronous  
        // recognition operation.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsync">
      <MemberSignature Language="C#" Value="public void RecognizeAsync (System.Speech.Recognition.RecognizeMode mode);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsync(valuetype System.Speech.Recognition.RecognizeMode mode) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync(System.Speech.Recognition.RecognizeMode)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="mode" Type="System.Speech.Recognition.RecognizeMode" />
      </Parameters>
      <Docs>
        <param name="mode">Indica se uma ou várias operações de reconhecimento devem ser executadas.</param>
        <summary>Executa uma ou mais operações de reconhecimento de fala assíncronas.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se `mode` é <xref:System.Speech.Recognition.RecognizeMode.Multiple>, o reconhecedor continua a executar operações assíncronas reconhecimento até que o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> método é chamado.  
  
 Durante uma chamada para esse método, o reconhecedor pode gerar os seguintes eventos:  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected>.  Gerado quando o reconhecedor detecta a entrada, ele pode identificar como fala.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>.  Gerado quando a entrada cria uma correspondência ambígua com uma das gramáticas ativas.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>. Gerado quando o reconhecedor Finaliza uma operação de reconhecimento.  
  
-   <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted>. Gerado quando um <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> operação concluída.  
  
 Para recuperar o resultado de uma operação assíncrona de reconhecimento, anexar um manipulador de eventos para o reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos. O reconhecedor gera esse evento sempre que ele concluir com êxito uma operação de reconhecimento síncronas ou assíncronas. Se o reconhecimento não foi bem-sucedida, o <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.Result%2A> propriedade <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> objeto, que você pode acessar o manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos, será `null`.  
  
 Uma operação assíncrona de reconhecimento pode falhar pelos seguintes motivos:  
  
-   Não a fala é detectada antes de expirarem os intervalos de tempo limite para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A> propriedades.  
  
-   O mecanismo de reconhecimento detecta fala, mas não encontrar correspondências em qualquer um dos seus carregados e habilitados <xref:System.Speech.Recognition.Grammar> objetos.  
  
 Para executar o reconhecimento síncrono, use um do <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> métodos.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala assíncrona básico. O exemplo cria um <xref:System.Speech.Recognition.DictationGrammar>, carrega-os em um reconhecedor de fala em processo e executa várias operações assíncronas de reconhecimento. Operações assíncronas são canceladas depois de 30 segundos. Manipuladores de eventos estão incluídos para demonstrar os eventos que o reconhecedor gera durante a operação.  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create a grammar for choosing cities for a flight.  
        Choices cities = new Choices(new string[] { "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I want to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Construct a Grammar object and load it to the recognizer.  
        Grammar cityChooser = new Grammar(gb);  
        cityChooser.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(cityChooser);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer and start asynchronous  
        // recognition.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        completed = false;  
        Console.WriteLine("Starting asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 30 seconds, and then cancel asynchronous recognition.  
        Thread.Sleep(TimeSpan.FromSeconds(30));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
        Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncCancel">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncCancel ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncCancel() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Encerra o reconhecimento assíncrono sem esperar que a operação de reconhecimento atual seja concluída.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Esse método imediatamente finaliza reconhecimento assíncrono. Se a operação assíncrona reconhecimento atual está recebendo a entrada, a entrada será truncada e a conclusão da operação com a entrada existente. Gera o reconhecedor o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> eventos quando uma operação assíncrona foi cancelada e define o <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> propriedade o <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> para `true`. Esse método cancela operações assíncronas iniciadas pelo <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> métodos.  
  
 Para interromper o reconhecimento assíncrono sem truncamento de entrada, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> método.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o uso de <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> método. O exemplo cria e carrega uma gramática de reconhecimento de fala, inicia uma operação assíncrona reconhecimento contínuos e, em seguida, pausa 2 segundos antes de ele cancela a operação. O reconhecedor recebe entrada do arquivo, c:\temp\audioinput\sample.wav. Manipuladores de eventos estão incluídos para demonstrar os eventos que o reconhecedor gera durante a operação.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then cancel the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncCancel();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizeAsyncStop">
      <MemberSignature Language="C#" Value="public void RecognizeAsyncStop ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RecognizeAsyncStop() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Interrompe o reconhecimento assíncrono após a conclusão da operação de reconhecimento atual.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Esse método finaliza reconhecimento assíncrono sem truncamento de entrada. Se a operação assíncrona reconhecimento atual está recebendo a entrada, o reconhecedor continua a aceitar a entrada até que a operação de reconhecimento atual seja concluída. Gera o reconhecedor o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted> eventos quando uma operação assíncrona está parada e define o <xref:System.ComponentModel.AsyncCompletedEventArgs.Cancelled%2A> propriedade o <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> para `true`. Esse método para operações assíncronas iniciadas pelo <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> métodos.  
  
 Para cancelar imediatamente o reconhecimento assíncrono com apenas a entrada existente, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncCancel%2A> método.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o uso de <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsyncStop%2A> método. O exemplo cria e carrega uma gramática de reconhecimento de fala, inicia uma operação assíncrona reconhecimento contínuos e, em seguida, pausa 2 segundos antes de parar a operação. O reconhecedor recebe entrada do arquivo, c:\temp\audioinput\sample.wav. Manipuladores de eventos estão incluídos para demonstrar os eventos que o reconhecedor gera durante a operação.  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace AsynchronousRecognition  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      // Create an in-process speech recognizer.  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        // Create and load a dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers.  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(  
            SpeechDetectedHandler);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(  
            SpeechHypothesizedHandler);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(  
            SpeechRecognitionRejectedHandler);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Begin asynchronous recognition from pre-recorded input.  
        recognizer.SetInputToWaveFile(@"c:\temp\audioinput\sample.wav");  
  
        completed = false;  
        Console.WriteLine("Begin continuing asynchronous recognition...");  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait 2 seconds and then stop the recognition operation.  
        Thread.Sleep(TimeSpan.FromSeconds(2));  
        recognizer.RecognizeAsyncStop();  
  
        // Wait for the operation to complete.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechDetected event.  
    static void SpeechDetectedHandler(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechDetectedHandler:");  
      Console.WriteLine(" - AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void SpeechHypothesizedHandler(  
      object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechHypothesizedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void SpeechRecognitionRejectedHandler(  
      object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognitionRejectedHandler:");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine(" In SpeechRecognizedHandler.");  
  
      string grammarName = "<not available>";  
      string resultText = "<not available>";  
      if (e.Result != null)  
      {  
        if (e.Result.Grammar != null)  
        {  
          grammarName = e.Result.Grammar.Name;  
        }  
        resultText = e.Result.Text;  
      }  
  
      Console.WriteLine(" - Grammar Name = {0}; Result Text = {1}",  
        grammarName, resultText);  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine(" In RecognizeCompletedHandler.");  
  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          " - Error occurred during recognition: {0}", e.Error);  
        return;  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine(" - asynchronous operation canceled.");  
      }  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          " - BabbleTimeout = {0}; InitialSilenceTimeout = {1}",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          " - AudioPosition = {0}; InputStreamEnded = {1}",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
      if (e.Result != null)  
      {  
        Console.WriteLine(  
          " - Grammar = {0}; Text = {1}; Confidence = {2}",  
          e.Result.Grammar.Name, e.Result.Text, e.Result.Confidence);  
      }  
      else  
      {  
        Console.WriteLine(" - No result.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizeCompleted">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizeCompletedEventArgs&gt; RecognizeCompleted" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizeCompletedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Acionado quando o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> finaliza uma operação assíncrona de reconhecimento.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O <xref:System.Speech.Recognition.SpeechRecognitionEngine> do objeto <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> método inicia uma operação assíncrona de reconhecimento. Quando o reconhecedor finaliza a operação assíncrona, ela gera esse evento.  
  
 Usar o manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> eventos, você pode acessar o <xref:System.Speech.Recognition.RecognitionResult> no <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> objeto. Se o reconhecimento não foi bem-sucedida, <xref:System.Speech.Recognition.RecognitionResult> será `null`. Para determinar se um tempo limite ou uma interrupção de entrada de áudio causado reconhecimento falhar, você pode acessar as propriedades de <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.BabbleTimeout%2A>, ou <xref:System.Speech.Recognition.RecognizeCompletedEventArgs.InputStreamEnded%2A>.  
  
 Consulte o <xref:System.Speech.Recognition.RecognizeCompletedEventArgs> classe para obter mais informações.  
  
 Para obter detalhes sobre os melhores candidatos rejeitado reconhecimento, anexar um manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> evento.  
  
 Quando você cria um <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> delegado, você identificar o método que manipulará o evento. Para associar o evento com o manipulador de eventos, adicione uma instância do representante ao evento. O manipulador de eventos é chamado sempre que o evento ocorre, a menos que você remova o representante. Para obter mais informações sobre delegados de manipulador de eventos, consulte [eventos e delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 O exemplo a seguir reconhece "frases como exibem a lista de artistas na categoria jazz" ou "gospel álbuns". O exemplo usa um manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted> evento para exibir informações sobre os resultados do reconhecimento no console do.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
        recognizer.LoadGrammarCompleted +=   
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted, error occurred during recognition: {0}", e.Error);  
        return;  
      }  
  
      if (e.InitialSilenceTimeout || e.BabbleTimeout)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: BabbleTimeout({0}), InitialSilenceTimeout({1}).",  
          e.BabbleTimeout, e.InitialSilenceTimeout);  
        return;  
      }  
  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine(  
          "RecognizeCompleted: AudioPosition({0}), InputStreamEnded({1}).",  
          e.AudioPosition, e.InputStreamEnded);  
      }  
  
      if (e.Result != null)  
      {  
        Console.WriteLine("RecognizeCompleted:");  
        Console.WriteLine("  Grammar: " + e.Result.Grammar.Name);  
        Console.WriteLine("  Recognized text: " + e.Result.Text);  
        Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
        Console.WriteLine("  Audio position: " + e.AudioPosition);  
      }  
  
      else  
      {  
        Console.WriteLine("RecognizeCompleted: No result.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded:  " + e.Grammar.Name);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizerAudioPosition">
      <MemberSignature Language="C#" Value="public TimeSpan RecognizerAudioPosition { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance valuetype System.TimeSpan RecognizerAudioPosition" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.TimeSpan</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtém o local atual do <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> na entrada de áudio que está processando.</summary>
        <value>A posição do reconhecedor de entrada de áudio está processando.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 A posição de áudio é específica para cada reconhecedor de fala. O valor zero de um fluxo de entrada é estabelecido quando ele está habilitado.  
  
 O <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> referências de propriedade de <xref:System.Speech.Recognition.SpeechRecognitionEngine> a posição do objeto dentro de sua entrada de áudio. Por outro lado, o <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propriedade faz referência a posição do dispositivo de entrada no seu fluxo de áudio gerado. Essas posições podem ser diferentes. Por exemplo, se tiver recebido o reconhecedor de entrada para o qual não tem ainda gerado um resultado de reconhecimento e o valor da <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> propriedade é menor que o valor da <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propriedade.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizerInfo">
      <MemberSignature Language="C#" Value="public System.Speech.Recognition.RecognizerInfo RecognizerInfo { get; }" />
      <MemberSignature Language="ILAsm" Value=".property instance class System.Speech.Recognition.RecognizerInfo RecognizerInfo" />
      <MemberSignature Language="DocId" Value="P:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerInfo" />
      <MemberType>Property</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Speech.Recognition.RecognizerInfo</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Obtém informações sobre a instância atual de <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" />.</summary>
        <value>Informações sobre o reconhecedor de fala atual.</value>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Para obter informações sobre todos os identificadores de fala instalados para o sistema atual, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.InstalledRecognizers%2A> método.  
  
   
  
## Examples  
 O exemplo a seguir obtém uma lista parcial de dados para o mecanismo de reconhecimento de fala no processo atual. Para obter mais informações, consulte <xref:System.Speech.Recognition.RecognizerInfo>.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace RecognitionEngine  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
        Console.WriteLine("Information for the current speech recognition engine:");  
        Console.WriteLine("  Name: {0}", recognizer.RecognizerInfo.Name);  
        Console.WriteLine("  Culture: {0}", recognizer.RecognizerInfo.Culture.ToString());  
        Console.WriteLine("  Description: {0}", recognizer.RecognizerInfo.Description);  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RecognizerUpdateReached">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt; RecognizerUpdateReached" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.RecognizerUpdateReachedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Gerado quando um execução <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> pausa para aceitar as modificações.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Os aplicativos devem usar <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> para pausar uma instância em execução de <xref:System.Speech.Recognition.SpeechRecognitionEngine> antes de modificar as configurações ou seus <xref:System.Speech.Recognition.Grammar> objetos. O <xref:System.Speech.Recognition.SpeechRecognitionEngine> gera esse evento quando ele estiver pronto para aceitar as modificações.  
  
 Por exemplo, enquanto o <xref:System.Speech.Recognition.SpeechRecognitionEngine> é pausado, você pode carregar, descarregar, habilitar e desabilitar <xref:System.Speech.Recognition.Grammar> objetos e modificar os valores para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> propriedades. Para obter mais informações, consulte o método <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A>.  
  
 Quando você cria um <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> delegado, você identificar o método que manipulará o evento. Para associar o evento com o manipulador de eventos, adicione uma instância do representante ao evento. O manipulador de eventos é chamado sempre que o evento ocorre, a menos que você remova o representante. Para obter mais informações sobre delegados de manipulador de eventos, consulte [eventos e delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 O exemplo a seguir mostra um aplicativo de console que carrega e descarrega <xref:System.Speech.Recognition.Grammar> objetos. O aplicativo usa o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> método para solicitar o reconhecimento de fala seja pausada para que ela possa receber uma atualização. O aplicativo, em seguida, carrega ou descarrega um <xref:System.Speech.Recognition.Grammar> objeto.  
  
 Em cada atualização, um manipulador para <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento grava o nome e o status de atualmente carregado <xref:System.Speech.Recognition.Grammar> objetos no console. Como as gramáticas são carregadas e descarregadas, o aplicativo primeiro reconhece os nomes de animais de farm, em seguida, os nomes de animais de farm e os nomes de frutas, em seguida, somente os nomes de frutas.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <MemberGroup MemberName="RequestRecognizerUpdate">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Solicita que o reconhecedor pausa para atualizar seu estado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Use esse método para sincronizar alterações para o reconhecedor. Por exemplo, se você carregar ou descarregar uma gramática de reconhecimento de voz enquanto o reconhecedor está processando a entrada, use este método e o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento para sincronizar o comportamento do seu aplicativo com o estado do reconhecedor de.  
  
 Quando este método é chamado, o reconhecedor de pausa ou conclusão de operações assíncronas e gera um <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos. Um <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> pode modificar o estado do reconhecedor entre operações de reconhecimento de manipulador de eventos. Ao lidar com <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> eventos, o reconhecedor de pausa até que retorna o manipulador de eventos.  
  
> [!NOTE]
>  Se a entrada para o reconhecedor for alterada antes de gerar o reconhecedor de <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento, a solicitação é descartado.  
  
 Quando este método é chamado:  
  
-   Se o reconhecedor não está processando a entrada, o reconhecedor gera imediatamente o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento.  
  
-   Se o reconhecedor está processando a entrada que consiste em silêncio ou ruídos de fundo, o identificador de pausa a operação de reconhecimento e gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento.  
  
-   Se o reconhecedor está processando a entrada não consistem em silêncio ou ruídos de fundo, o reconhecedor conclui a operação de reconhecimento e, em seguida, gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento.  
  
 Enquanto o reconhecedor está tratando o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento:  
  
-   O reconhecedor não processa a entrada e o valor da <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> propriedade permanece o mesmo.  
  
-   O reconhecedor continuará a coletar de entrada e o valor da <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> propriedade pode ser alterada.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Solicita que o reconhecedor pausa para atualizar seu estado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Quando o reconhecedor gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento, o <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> propriedade do <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> é `null`.  
  
 Para fornecer um token de usuário, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> método. Para especificar um deslocamento de posição de áudio, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> método.  
  
   
  
## Examples  
 O exemplo a seguir mostra um aplicativo de console que carrega e descarrega <xref:System.Speech.Recognition.Grammar> objetos. O aplicativo usa o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> método para solicitar o reconhecimento de fala seja pausada para que ela possa receber uma atualização. O aplicativo, em seguida, carrega ou descarrega um <xref:System.Speech.Recognition.Grammar> objeto.  
  
 Em cada atualização, um manipulador para <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento grava o nome e o status de atualmente carregado <xref:System.Speech.Recognition.Grammar> objetos no console. Como as gramáticas são carregadas e descarregadas, o aplicativo primeiro reconhece os nomes de animais de farm, em seguida, os nomes de animais de farm e os nomes de frutas, em seguida, somente os nomes de frutas.  
  
```  
using System;  
using System.Speech.Recognition;  
using System.Collections.Generic;  
using System.Threading;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    private static SpeechRecognitionEngine recognizer;  
    public static void Main(string[] args)  
    {  
  
      // Initialize an in-process speech recognition engine and configure its input.  
      using (recognizer = new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Create the first grammar - Farm.  
        Choices animals = new Choices(new string[] { "cow", "pig", "goat" });  
        GrammarBuilder farm = new GrammarBuilder(animals);  
        Grammar farmAnimals = new Grammar(farm);  
        farmAnimals.Name = "Farm";  
  
        // Create the second grammar - Fruit.  
        Choices fruit = new Choices(new string[] { "apples", "peaches", "oranges" });  
        GrammarBuilder favorite = new GrammarBuilder(fruit);  
        Grammar favoriteFruit = new Grammar(favorite);  
        favoriteFruit.Name = "Fruit";  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizerUpdateReached +=  
          new EventHandler<RecognizerUpdateReachedEventArgs>(recognizer_RecognizerUpdateReached);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the Farm grammar.  
        recognizer.LoadGrammar(farmAnimals);  
  
        // Start asynchronous, continuous recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
        Console.WriteLine("Starting asynchronous, continuous recognition");  
        Console.WriteLine("  Farm grammar is loaded and enabled.");  
  
        // Pause to recognize farm animals.  
        Thread.Sleep(7000);  
        Console.WriteLine();  
  
        // Request an update and load the Fruit grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.LoadGrammarAsync(favoriteFruit);  
        Thread.Sleep(7000);  
  
        // Request an update and unload the Farm grammar.  
        recognizer.RequestRecognizerUpdate();  
        recognizer.UnloadGrammar(farmAnimals);  
        Thread.Sleep(7000);  
      }  
  
      // Keep the console window open.  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // At the update, get the names and enabled status of the currently loaded grammars.  
    public static void recognizer_RecognizerUpdateReached(  
      object sender, RecognizerUpdateReachedEventArgs e)  
    {  
      Console.WriteLine();  
      Console.WriteLine("Update reached:");  
      Thread.Sleep(1000);  
  
      string qualifier;  
      List<Grammar> grammars = new List<Grammar>(recognizer.Grammars);  
      foreach (Grammar g in grammars)  
      {  
        qualifier = (g.Enabled) ? "enabled" : "disabled";  
        Console.WriteLine("  {0} grammar is loaded and {1}.",  
        g.Name, qualifier);  
      }  
    }  
  
    // Write the text of the recognized phrase to the console.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("    Speech recognized: " + e.Result.Text);  
    }  
  
    // Write a message to the console when recognition fails.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("    Recognition attempt failed");  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
      </Parameters>
      <Docs>
        <param name="userToken">Informações definidas pelo usuário que contém informações para a operação.</param>
        <summary>Solicita que o reconhecedor pausa para atualizar seu estado e fornece um token de usuário para o evento associado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Quando o reconhecedor gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento, o <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> propriedade do <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contém o valor da `userToken` parâmetro.  
  
 Para especificar um deslocamento de posição de áudio, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> método.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="RequestRecognizerUpdate">
      <MemberSignature Language="C#" Value="public void RequestRecognizerUpdate (object userToken, TimeSpan audioPositionAheadToRaiseUpdate);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void RequestRecognizerUpdate(object userToken, valuetype System.TimeSpan audioPositionAheadToRaiseUpdate) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate(System.Object,System.TimeSpan)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="userToken" Type="System.Object" />
        <Parameter Name="audioPositionAheadToRaiseUpdate" Type="System.TimeSpan" />
      </Parameters>
      <Docs>
        <param name="userToken">Informações definidas pelo usuário que contém informações para a operação.</param>
        <param name="audioPositionAheadToRaiseUpdate">O deslocamento do atual <see cref="P:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition" /> para atrasar a solicitação.</param>
        <summary>Solicita que o reconhecedor pausa para atualizar seu estado e fornece um deslocamento e um token de usuário para o evento associado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O reconhecedor não iniciará a solicitação de atualização do reconhecedor até que o reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerAudioPosition%2A> é igual a atual <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> mais `audioPositionAheadToRaiseUpdate`.  
  
 Quando o reconhecedor gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizerUpdateReached> evento, o <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs.UserToken%2A> propriedade do <xref:System.Speech.Recognition.RecognizerUpdateReachedEventArgs> contém o valor da `userToken` parâmetro.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SetInputToAudioStream">
      <MemberSignature Language="C#" Value="public void SetInputToAudioStream (System.IO.Stream audioSource, System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToAudioStream(class System.IO.Stream audioSource, class System.Speech.AudioFormat.SpeechAudioFormatInfo audioFormat) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToAudioStream(System.IO.Stream,System.Speech.AudioFormat.SpeechAudioFormatInfo)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
        <Parameter Name="audioFormat" Type="System.Speech.AudioFormat.SpeechAudioFormatInfo" />
      </Parameters>
      <Docs>
        <param name="audioSource">O fluxo de entrada de áudio.</param>
        <param name="audioFormat">O formato da entrada de áudio.</param>
        <summary>Configura o objeto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> para receber entrada de um fluxo de áudio.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se o reconhecedor atinge o final do fluxo de entrada durante uma operação de reconhecimento, a operação de reconhecimento finaliza com a entrada disponível. Todas as operações subsequentes de reconhecimento podem gerar uma exceção, a menos que você atualizar a entrada para o reconhecedor.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala básico. O exemplo usa a entrada de um arquivo de áudio, example.wav, que contém as frases, "teste testando um dois três" e "mister cooper", separados por uma pausa. O exemplo gera a saída a seguir.  
  
```  
  
Starting asynchronous recognition...  
  Recognized text =  Testing testing 123  
  Recognized text =  Mr. Cooper  
  End of stream encountered.  
Done.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.IO;  
using System.Speech.AudioFormat;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace InputExamples  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition is complete.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
        recognizer.SetInputToAudioStream(  
          File.OpenRead(@"c:\temp\audioinput\example.wav"),  
          new SpeechAudioFormatInfo(  
            44100, AudioBitsPerSample.Sixteen, AudioChannel.Mono));  
  
        // Attach event handlers.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Perform recognition of the whole file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
          e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SetInputToDefaultAudioDevice">
      <MemberSignature Language="C#" Value="public void SetInputToDefaultAudioDevice ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToDefaultAudioDevice() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToDefaultAudioDevice" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Configura o objeto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> para receber a entrada do dispositivo de áudio padrão.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o reconhecimento de fala básico. O exemplo usa a saída do dispositivo de áudio padrão, executa várias operações assíncronas de reconhecimento e sai quando um usuário utters a frase "exit".  
  
```csharp  
  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
using System.Threading;  
  
namespace DefaultInput  
{  
  class Program  
  {  
    // Indicate whether asynchronous recognition has finished.  
    static bool completed;  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
  
        // Create and load the exit grammar.  
        Grammar exitGrammar = new Grammar(new GrammarBuilder("exit"));  
        exitGrammar.Name = "Exit Grammar";  
        recognizer.LoadGrammar(exitGrammar);  
  
        // Create and load the dictation grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
        recognizer.LoadGrammar(dictation);  
  
        // Attach event handlers to the recognizer.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(  
            SpeechRecognizedHandler);  
        recognizer.RecognizeCompleted +=  
          new EventHandler<RecognizeCompletedEventArgs>(  
            RecognizeCompletedHandler);  
  
        // Assign input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Begin asynchronous recognition.  
        Console.WriteLine("Starting recognition...");  
        completed = false;  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Wait for recognition to finish.  
        while (!completed)  
        {  
          Thread.Sleep(333);  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    static void SpeechRecognizedHandler(  
      object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized:");  
      string grammarName = "<not available>";  
      if (e.Result.Grammar.Name != null &&  
        !e.Result.Grammar.Name.Equals(string.Empty))  
      {  
        grammarName = e.Result.Grammar.Name;  
      }  
      Console.WriteLine("    {0,-17} - {1}",  
        grammarName, e.Result.Text);  
  
      if (grammarName.Equals("Exit Grammar"))  
      {  
        ((SpeechRecognitionEngine)sender).RecognizeAsyncCancel();  
      }  
    }  
  
    static void RecognizeCompletedHandler(  
      object sender, RecognizeCompletedEventArgs e)  
    {  
      Console.WriteLine("  Recognition completed.");  
      completed = true;  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SetInputToNull">
      <MemberSignature Language="C#" Value="public void SetInputToNull ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToNull() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToNull" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Desabilita a entrada para o reconhecedor de fala.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Configurar o <xref:System.Speech.Recognition.SpeechRecognitionEngine> objeto para nenhuma entrada ao usar o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A> e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> métodos, ou quando um mecanismo de reconhecimento temporariamente off-line.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveFile">
      <MemberSignature Language="C#" Value="public void SetInputToWaveFile (string path);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveFile(string path) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveFile(System.String)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="path" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="path">O caminho do arquivo a ser usado como entrada.</param>
        <summary>Configura o objeto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> para receber entrada de um arquivo de formato de áudio Waveform (.wav).</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se o reconhecedor atinge o final do arquivo de entrada durante uma operação de reconhecimento, a operação de reconhecimento finaliza com a entrada disponível. Todas as operações subsequentes de reconhecimento podem gerar uma exceção, a menos que você atualizar a entrada para o reconhecedor.  
  
   
  
## Examples  
 O exemplo a seguir executa o reconhecimento de um arquivo de áudio e grava o texto reconhecido para o console.  
  
```  
using System;  
using System.IO;  
using System.Speech.Recognition;  
using System.Speech.AudioFormat;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static bool completed;  
  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create and load a grammar.  
        Grammar dictation = new DictationGrammar();  
        dictation.Name = "Dictation Grammar";  
  
        recognizer.LoadGrammar(dictation);  
  
        // Configure the input to the recognizer.  
recognizer.SetInputToWaveFile(@"c:\temp\SampleWAVInput.wav");  
  
        // Attach event handlers for the results of recognition.  
        recognizer.SpeechRecognized +=   
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.RecognizeCompleted +=   
          new EventHandler<RecognizeCompletedEventArgs>(recognizer_RecognizeCompleted);  
  
        // Perform recognition on the entire file.  
        Console.WriteLine("Starting asynchronous recognition...");  
        completed = false;  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        while (!completed)  
        {  
          Console.ReadLine();  
        }  
        Console.WriteLine("Done.");  
      }  
  
      Console.WriteLine();  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      if (e.Result != null && e.Result.Text != null)  
      {  
        Console.WriteLine("  Recognized text =  {0}", e.Result.Text);  
      }  
      else  
      {  
        Console.WriteLine("  Recognized text not available.");  
      }  
    }  
  
    // Handle the RecognizeCompleted event.  
    static void recognizer_RecognizeCompleted(object sender, RecognizeCompletedEventArgs e)  
    {  
      if (e.Error != null)  
      {  
        Console.WriteLine("  Error encountered, {0}: {1}",  
        e.Error.GetType().Name, e.Error.Message);  
      }  
      if (e.Cancelled)  
      {  
        Console.WriteLine("  Operation cancelled.");  
      }  
      if (e.InputStreamEnded)  
      {  
        Console.WriteLine("  End of stream encountered.");  
      }  
      completed = true;  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SetInputToWaveStream">
      <MemberSignature Language="C#" Value="public void SetInputToWaveStream (System.IO.Stream audioSource);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void SetInputToWaveStream(class System.IO.Stream audioSource) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.SetInputToWaveStream(System.IO.Stream)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="audioSource" Type="System.IO.Stream" />
      </Parameters>
      <Docs>
        <param name="audioSource">O fluxo que contém os dados de áudio.</param>
        <summary>Configura o objeto <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> para receber a entrada de um fluxo que contém dados de formato de áudio Waveform (.wav).</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se o reconhecedor atinge o final do fluxo de entrada durante uma operação de reconhecimento, a operação de reconhecimento finaliza com a entrada disponível. Todas as operações subsequentes de reconhecimento podem gerar uma exceção, a menos que você atualizar a entrada para o reconhecedor.  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SpeechDetected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechDetectedEventArgs&gt; SpeechDetected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechDetectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Gerado quando o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> detecta a entradas que podem ser identificadas como fala.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Cada reconhecedor de fala tem um algoritmo para distinguir entre silêncio e fala. Quando o <xref:System.Speech.Recognition.SpeechRecognitionEngine> executa uma operação de reconhecimento de fala, ele gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> evento quando seu algoritmo identifica a entrada como fala. O <xref:System.Speech.Recognition.SpeechDetectedEventArgs.AudioPosition%2A> propriedade associado <xref:System.Speech.Recognition.SpeechDetectedEventArgs> objeto indica o local no fluxo de entrada em que o o reconhecedor de fala detectado. O <xref:System.Speech.Recognition.SpeechRecognitionEngine> gera o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> evento antes de gerar qualquer uma da <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized>, ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> eventos.  
  
 Para obter mais informações, consulte o <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> métodos.  
  
 Quando você cria um <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> delegado, você identificar o método que manipulará o evento. Para associar o evento com o manipulador de eventos, adicione uma instância do representante ao evento. O manipulador de eventos é chamado sempre que o evento ocorre, a menos que você remova o representante. Para obter mais informações sobre delegados de manipulador de eventos, consulte [eventos e delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 O exemplo a seguir é parte de um aplicativo de console para a escolha de cidades de origem e destino para um voo. O aplicativo reconhece frases como "Eu quero Deslizar Miami para Chicago."  O exemplo usa o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechDetected> evento para relatório de <xref:System.Speech.Recognition.SpeechRecognitionEngine.AudioPosition%2A> a cada hora fala é detectada.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        Choices cities = new Choices(new string[] {   
          "Los Angeles", "New York", "Chicago", "San Francisco", "Miami", "Dallas" });  
  
        GrammarBuilder gb = new GrammarBuilder();  
        gb.Append("I would like to fly from");  
        gb.Append(cities);  
        gb.Append("to");  
        gb.Append(cities);  
  
        // Create a Grammar object and load it to the recognizer.  
        Grammar g = new Grammar(gb);  
        g.Name = ("City Chooser");  
        recognizer.LoadGrammarAsync(g);  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechDetected +=  
          new EventHandler<SpeechDetectedEventArgs>(recognizer_SpeechDetected);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechDetected event.  
    static void recognizer_SpeechDetected(object sender, SpeechDetectedEventArgs e)  
    {  
      Console.WriteLine("  Speech detected at AudioPosition = {0}", e.AudioPosition);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("  Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SpeechHypothesized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechHypothesizedEventArgs&gt; SpeechHypothesized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechHypothesizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Gerada quando o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> tiver reconhecido uma palavra ou palavras que podem ser um componente de várias frases completas em uma gramática.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O <xref:System.Speech.Recognition.SpeechRecognitionEngine> gera vários <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> eventos quando ele tenta identificar uma frase de entrada. Você pode acessar o texto de frases parcialmente reconhecidos no <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> propriedade do <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> objeto no manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> evento. Normalmente, manipular esses eventos só é útil para depuração.  
  
 <xref:System.Speech.Recognition.SpeechHypothesizedEventArgs> deriva de <xref:System.Speech.Recognition.RecognitionEventArgs>.  
  
 Para obter mais informações, consulte o <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriedade e o <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A> métodos.  
  
 Quando você cria um <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> delegado, você identificar o método que manipulará o evento. Para associar o evento com o manipulador de eventos, adicione uma instância do representante ao evento. O manipulador de eventos é chamado sempre que o evento ocorre, a menos que você remova o representante. Para obter mais informações sobre delegados de manipulador de eventos, consulte [eventos e delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 O exemplo a seguir reconhece frases como "Exibir a lista de artistas na categoria jazz". O exemplo usa o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized> evento para exibir os fragmentos de frase incompletas no console do conforme eles são reconhecidos.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine())  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display the list of");  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the");  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category.");  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechHypothesized +=  
          new EventHandler<SpeechHypothesizedEventArgs>(recognizer_SpeechHypothesized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start asynchronous recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechHypothesized event.  
    static void recognizer_SpeechHypothesized(object sender, SpeechHypothesizedEventArgs e)  
    {  
      Console.WriteLine("Speech hypothesized: " + e.Result.Text);  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine();   
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognitionRejected">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt; SpeechRecognitionRejected" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognitionRejectedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Gerado quando o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> recebe uma entrada que não corresponde a nenhum de seus objetos <see cref="T:System.Speech.Recognition.Grammar" /> carregados e habilitados.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 O reconhecedor gera esse evento se determinar que entrada não coincide com confiança suficiente qualquer um dos seus carregados e habilitados <xref:System.Speech.Recognition.Grammar> objetos. O <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> propriedade o <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contém o rejeitado <xref:System.Speech.Recognition.RecognitionResult> objeto. Você pode usar o manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> evento para recuperar o reconhecimento <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> que foram rejeitadas e seus <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> pontuações.  
  
 Se seu aplicativo estiver usando um <xref:System.Speech.Recognition.SpeechRecognitionEngine> instância, você pode modificar o nível de confiança no qual fala entrada é aceita ou rejeitada com um do <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> métodos. Você pode modificar como o reconhecimento de fala responde a não fala de entrada usando o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriedades.  
  
 Quando você cria um <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> delegado, você identificar o método que manipulará o evento. Para associar o evento com o manipulador de eventos, adicione uma instância do representante ao evento. O manipulador de eventos é chamado sempre que o evento ocorre, a menos que você remova o representante. Para obter mais informações sobre delegados de manipulador de eventos, consulte [eventos e delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 O exemplo a seguir reconhece "frases como exibem a lista de artistas na categoria jazz" ou "gospel álbuns". O exemplo usa um manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected> evento para exibir uma notificação no console quando a fala de entrada não corresponde ao conteúdo da gramática com suficiente <xref:System.Speech.Recognition.RecognizedPhrase.Confidence%2A> para produzir um reconhecimento bem-sucedido. O manipulador também exibe os resultados de reconhecimento <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> que foram rejeitadas devido pontuações de confiança baixa.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer =  
         new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
  
        // Create a grammar.  
        //  Create lists of alternative choices.  
        Choices listTypes = new Choices(new string[] { "albums", "artists" });  
        Choices genres = new Choices(new string[] {   
          "blues", "classical", "gospel", "jazz", "rock" });  
  
        //  Create a GrammarBuilder object and assemble the grammar components.  
        GrammarBuilder mediaMenu = new GrammarBuilder("Display");  
        mediaMenu.Append("the list of", 0, 1);  
        mediaMenu.Append(listTypes);  
        mediaMenu.Append("in the", 0, 1);  
        mediaMenu.Append(genres);  
        mediaMenu.Append("category", 0, 1);  
  
        //  Build a Grammar object from the GrammarBuilder.  
        Grammar mediaMenuGrammar = new Grammar(mediaMenu);  
        mediaMenuGrammar.Name = "Media Chooser";  
  
        // Attach event handlers.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
        recognizer.SpeechRecognitionRejected +=  
          new EventHandler<SpeechRecognitionRejectedEventArgs>(recognizer_SpeechRecognitionRejected);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(mediaMenuGrammar);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync(RecognizeMode.Multiple);  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the SpeechRecognitionRejected event.  
    static void recognizer_SpeechRecognitionRejected(object sender, SpeechRecognitionRejectedEventArgs e)  
    {  
      Console.WriteLine("Speech input was rejected.");  
      foreach (RecognizedPhrase phrase in e.Result.Alternates)  
      {  
      Console.WriteLine("  Rejected phrase: " + phrase.Text);  
      Console.WriteLine("  Confidence score: " + phrase.Confidence);  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized: " + e.Result.Text);  
      Console.WriteLine("  Confidence score: " + e.Result.Confidence);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="SpeechRecognized">
      <MemberSignature Language="C#" Value="public event EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized;" />
      <MemberSignature Language="ILAsm" Value=".event class System.EventHandler`1&lt;class System.Speech.Recognition.SpeechRecognizedEventArgs&gt; SpeechRecognized" />
      <MemberSignature Language="DocId" Value="E:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized" />
      <MemberType>Event</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.EventHandler&lt;System.Speech.Recognition.SpeechRecognizedEventArgs&gt;</ReturnType>
      </ReturnValue>
      <Docs>
        <summary>Gerado quando o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> recebe uma entrada que corresponde a um de seus objetos <see cref="T:System.Speech.Recognition.Grammar" /> carregados e habilitados.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Você pode iniciar uma operação de reconhecimento usando um do <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A> ou <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A> métodos. Gera o reconhecedor de <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento se determinar que entrada corresponde a um dos seus carregado <xref:System.Speech.Recognition.Grammar> objetos com um nível suficiente de confiança para constituir reconhecimento. O <xref:System.Speech.Recognition.RecognitionEventArgs.Result%2A> propriedade o <xref:System.Speech.Recognition.SpeechRecognitionRejectedEventArgs> contém o aceito <xref:System.Speech.Recognition.RecognitionResult> objeto. Manipuladores de <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos podem obter a frase reconhecida como uma lista de reconhecimento <xref:System.Speech.Recognition.RecognitionResult.Alternates%2A> com pontuações de confiança inferiores.  
  
 Se seu aplicativo estiver usando um <xref:System.Speech.Recognition.SpeechRecognitionEngine> instância, você pode modificar o nível de confiança no qual fala entrada é aceita ou rejeitada com um do <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> métodos.  Você pode modificar como o reconhecimento de fala responde a não fala de entrada usando o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriedades.  
  
 Quando o reconhecedor recebe entrada que corresponde a uma gramática, o <xref:System.Speech.Recognition.Grammar> objeto pode gerar seu <xref:System.Speech.Recognition.Grammar.SpeechRecognized> eventos. O <xref:System.Speech.Recognition.Grammar> do objeto <xref:System.Speech.Recognition.Grammar.SpeechRecognized> é gerado antes do reconhecedor de fala <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos. As tarefas específicas a uma gramática específica sempre devem ser realizadas por um manipulador para o <xref:System.Speech.Recognition.Grammar.SpeechRecognized> evento.  
  
 Quando você cria um <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> delegado, você identificar o método que manipulará o evento. Para associar o evento com o manipulador de eventos, adicione uma instância do representante ao evento. O manipulador de eventos é chamado sempre que o evento ocorre, a menos que você remova o representante. Para obter mais informações sobre delegados de manipulador de eventos, consulte [eventos e delegados](http://go.microsoft.com/fwlink/?LinkId=162418).  
  
   
  
## Examples  
 O exemplo a seguir é parte de um aplicativo de console que cria a gramática de reconhecimento de fala, construções um <xref:System.Speech.Recognition.Grammar> objeto e, em seguida, carrega no <xref:System.Speech.Recognition.SpeechRecognitionEngine> para executar o reconhecimento. O exemplo demonstra fala para um <xref:System.Speech.Recognition.SpeechRecognitionEngine>, os resultados do reconhecimento associados e os eventos associados emitindo o reconhecedor de fala.  
  
 Falado entrada, como "Eu quero surgir de Chicago para Miami" irá disparar um <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos. Falando a frase "Surgir me de Houston para Chicago" não vai disparar um <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> eventos.  
  
 O exemplo usa um manipulador para o <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized> evento para exibir com êxito reconhecido frases e a semântica que eles contêm no console do.  
  
```  
using System;  
using System.Speech.Recognition;  
  
namespace SampleRecognition  
{  
  class Program  
  {  
    static void Main(string[] args)  
  
    // Initialize an in-process speech recognition engine.  
    {  
      using (SpeechRecognitionEngine recognizer = new SpeechRecognitionEngine())  
      {  
  
        // Create SemanticResultValue objects that contain cities and airport codes.  
        SemanticResultValue chicago = new SemanticResultValue("Chicago", "ORD");  
        SemanticResultValue boston = new SemanticResultValue("Boston", "BOS");  
        SemanticResultValue miami = new SemanticResultValue("Miami", "MIA");  
        SemanticResultValue dallas = new SemanticResultValue("Dallas", "DFW");  
  
        // Create a Choices object and add the SemanticResultValue objects, using  
        // implicit conversion from SemanticResultValue to GrammarBuilder  
        Choices cities = new Choices();  
        cities.Add(new Choices(new GrammarBuilder[] { chicago, boston, miami, dallas }));  
  
        // Build the phrase and add SemanticResultKeys.  
        GrammarBuilder chooseCities = new GrammarBuilder();  
        chooseCities.Append("I want to fly from");  
        chooseCities.Append(new SemanticResultKey("origin", cities));  
        chooseCities.Append("to");  
        chooseCities.Append(new SemanticResultKey("destination", cities));  
  
        // Build a Grammar object from the GrammarBuilder.  
        Grammar bookFlight = new Grammar(chooseCities);  
        bookFlight.Name = "Book Flight";  
  
        // Add a handler for the LoadGrammarCompleted event.  
        recognizer.LoadGrammarCompleted +=  
          new EventHandler<LoadGrammarCompletedEventArgs>(recognizer_LoadGrammarCompleted);  
  
        // Add a handler for the SpeechRecognized event.  
        recognizer.SpeechRecognized +=  
          new EventHandler<SpeechRecognizedEventArgs>(recognizer_SpeechRecognized);  
  
        // Load the grammar object to the recognizer.  
        recognizer.LoadGrammarAsync(bookFlight);  
  
        // Set the input to the recognizer.  
        recognizer.SetInputToDefaultAudioDevice();  
  
        // Start recognition.  
        recognizer.RecognizeAsync();  
  
        // Keep the console window open.  
        Console.ReadLine();  
      }  
    }  
  
    // Handle the LoadGrammarCompleted event.  
    static void recognizer_LoadGrammarCompleted(object sender, LoadGrammarCompletedEventArgs e)  
    {  
      Console.WriteLine("Grammar loaded: " + e.Grammar.Name);  
      Console.WriteLine();  
    }  
  
    // Handle the SpeechRecognized event.  
    static void recognizer_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)  
    {  
      Console.WriteLine("Speech recognized:  " + e.Result.Text);  
      Console.WriteLine();  
      Console.WriteLine("Semantic results:");  
      Console.WriteLine("  The flight origin is " + e.Result.Semantics["origin"].Value);  
      Console.WriteLine("  The flight destination is " + e.Result.Semantics["destination"].Value);  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="UnloadAllGrammars">
      <MemberSignature Language="C#" Value="public void UnloadAllGrammars ();" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadAllGrammars() cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters />
      <Docs>
        <summary>Descarrega todos <see cref="T:System.Speech.Recognition.Grammar" /> objetos a partir do reconhecedor.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se o reconhecedor está carregando no momento um <xref:System.Speech.Recognition.Grammar> de forma assíncrona, este método espera até que o <xref:System.Speech.Recognition.Grammar> são carregados, antes de ele descarrega todos o <xref:System.Speech.Recognition.Grammar> objetos do <xref:System.Speech.Recognition.SpeechRecognitionEngine> instância.  
  
 Para descarregar uma gramática específica, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar%2A> método.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o carregamento síncrono e descarregamento de gramáticas de reconhecimento de fala.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
      </Docs>
    </Member>
    <Member MemberName="UnloadGrammar">
      <MemberSignature Language="C#" Value="public void UnloadGrammar (System.Speech.Recognition.Grammar grammar);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UnloadGrammar(class System.Speech.Recognition.Grammar grammar) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UnloadGrammar(System.Speech.Recognition.Grammar)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="grammar" Type="System.Speech.Recognition.Grammar" />
      </Parameters>
      <Docs>
        <param name="grammar">O objeto de gramática descarregar.</param>
        <summary>Descarrega um especificado <see cref="T:System.Speech.Recognition.Grammar" /> de objeto o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> instância.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Se o reconhecedor está em execução, os aplicativos devem usar <xref:System.Speech.Recognition.SpeechRecognitionEngine.RequestRecognizerUpdate%2A> para pausar o <xref:System.Speech.Recognition.SpeechRecognitionEngine> instância antes de carregamento, descarregamento, habilitar ou desabilitar um <xref:System.Speech.Recognition.Grammar> objeto. Para descarregar todos os <xref:System.Speech.Recognition.Grammar> objetos, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.UnloadAllGrammars%2A> método.  
  
   
  
## Examples  
 O exemplo a seguir mostra parte de um aplicativo de console que demonstra o carregamento síncrono e descarregamento de gramáticas de reconhecimento de fala.  
  
```  
Loading grammars...  
Loaded grammars:  
 - Grammar1  
 - Grammar2  
 - Grammar3  
  
Unloading Grammar1...  
Loaded grammars:  
 - Grammar2  
 - Grammar3  
  
Unloading all grammars...  
No grammars loaded.  
  
Press any key to exit...  
```  
  
```csharp  
  
using System;  
using System.Collections.Generic;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace UnloadGrammars  
{  
  class Program  
  {  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new CultureInfo("en-US")))  
      {  
        Console.WriteLine("Loading grammars...");  
  
        // Create and load a number of grammars.  
        Grammar grammar1 = new Grammar(new GrammarBuilder("first grammar"));  
        grammar1.Name = "Grammar1";  
        recognizer.LoadGrammar(grammar1);  
  
        Grammar grammar2 = new Grammar(new GrammarBuilder("second grammar"));  
        grammar2.Name = "Grammar2";  
        recognizer.LoadGrammar(grammar2);  
  
        Grammar grammar3 = new Grammar(new GrammarBuilder("third grammar"));  
        grammar3.Name = "Grammar3";  
        recognizer.LoadGrammar(grammar3);  
  
        // List the recognizer's loaded grammars.  
        ListGrammars(recognizer);  
  
        // Unload one grammar and list the loaded grammars.  
        Console.WriteLine("Unloading Grammar1...");  
        recognizer.UnloadGrammar(grammar1);  
        ListGrammars(recognizer);  
  
        // Unload all grammars and list the loaded grammars.  
        Console.WriteLine("Unloading all grammars...");  
        recognizer.UnloadAllGrammars();  
        ListGrammars(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListGrammars(SpeechRecognitionEngine recognizer)  
    {  
      // Make a copy of the recognizer's grammar collection.  
      List<Grammar> loadedGrammars = new List<Grammar>(recognizer.Grammars);  
  
      if (loadedGrammars.Count > 0)  
      {  
        Console.WriteLine("Loaded grammars:");  
        foreach (Grammar g in recognizer.Grammars)  
        {  
          Console.WriteLine(" - {0}", g.Name);  
        }  
      }  
      else  
      {  
        Console.WriteLine("No grammars loaded.");  
      }  
      Console.WriteLine();  
    }  
  }  
}  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="Grammar" /> é <see langword="null" />.</exception>
        <exception cref="T:System.InvalidOperationException">A gramática não está carregada neste reconhecedor ou neste reconhecedor atualmente está carregando a gramática de forma assíncrona.</exception>
      </Docs>
    </Member>
    <MemberGroup MemberName="UpdateRecognizerSetting">
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <Docs>
        <summary>Atualiza o valor de uma configuração do reconhecedor.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Configurações do reconhecedor podem conter a cadeia de caracteres, inteiro de 64 bits ou dados de endereço de memória. A tabela a seguir descreve as configurações que são definidas para uma API do Microsoft Speech (SAPI)-reconhecedor compatível. As configurações a seguir devem ter o mesmo intervalo para cada identificador que suporta a configuração. Um identificador compatível com SAPI não é necessário para dar suporte a essas configurações e pode dar suporte a outras configurações.  
  
|Nome|Descrição|  
|----------|-----------------|  
|`ResourceUsage`|Especifica o consumo de CPU do reconhecedor. O intervalo é de 0 a 100. O valor padrão é 50.|  
|`ResponseSpeed`|Indica o comprimento de silêncio no final da entrada ambíguo antes do reconhecedor de fala conclui uma operação de reconhecimento. O intervalo é de 0 a 10.000 milissegundos (ms). Essa configuração corresponde do reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A> propriedade. Padrão = 150 ms.|  
|`ComplexResponseSpeed`|Indica o comprimento de latência em milissegundos (ms) ao final da entrada ambígua antes do reconhecedor de fala conclui uma operação de reconhecimento. O intervalo é de 0 a 10.000 ms. Essa configuração corresponde do reconhecedor <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriedade. Padrão = 500 ms.|  
|`AdaptationOn`|Indica se a adaptação do modelo acústico está ON (valor = `1`) ou OFF (valor = `0`). O valor padrão é `1` (ON).|  
|`PersistedBackgroundAdaptation`|Indica se a adaptação de plano de fundo é ON (valor = `1`) ou OFF (valor = `0`), e persistir a configuração no registro. O valor padrão é `1` (ON).|  
  
 Para retornar uma das configurações do reconhecedor, use o <xref:System.Speech.Recognition.SpeechRecognitionEngine.QueryRecognizerSetting%2A> método.  
  
 Com exceção de `PersistedBackgroundAdaptation`, valores de propriedade definidos usando o <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> métodos permanecem em vigor apenas para a instância atual do <xref:System.Speech.Recognition.SpeechRecognitionEngine>, após o qual reverter as configurações padrão.  
  
 Você pode modificar como o reconhecimento de fala responde a não fala de entrada usando o <xref:System.Speech.Recognition.SpeechRecognitionEngine.BabbleTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.InitialSilenceTimeout%2A>, <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeout%2A>, e <xref:System.Speech.Recognition.SpeechRecognitionEngine.EndSilenceTimeoutAmbiguous%2A> propriedades.  
  
 ]]></format>
        </remarks>
      </Docs>
    </MemberGroup>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, int updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, int32 updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.Int32)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.Int32" />
      </Parameters>
      <Docs>
        <param name="settingName">O nome da configuração a ser atualizada.</param>
        <param name="updatedValue">O novo valor para a configuração.</param>
        <summary>Atualiza a configuração especificada para o <see cref="T:System.Speech.Recognition.SpeechRecognitionEngine" /> com o valor inteiro especificado.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Com exceção de `PersistedBackgroundAdaptation`, valores de propriedade definidos usando o <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> método permanecem em vigor apenas para a instância atual do <xref:System.Speech.Recognition.SpeechRecognitionEngine>, após o qual reverter as configurações padrão. Consulte <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> para obter descrições de configurações com suporte.  
  
   
  
## Examples  
 O exemplo a seguir é parte de um aplicativo de console que gera os valores para um número das configurações definidas para o reconhecedor que oferece suporte a localidade en-US. O exemplo atualiza as configurações de nível de confiança e, em seguida, consulta o reconhecedor para verificar os valores atualizados. O exemplo gera a saída a seguir.  
  
```  
Settings for recognizer MS-1033-80-DESK:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 150  
  ComplexResponseSpeed           = 500  
  AdaptationOn                   = 1  
  PersistedBackgroundAdaptation  = 1  
  
Updated settings:  
  
  ResourceUsage                  is not supported by this recognizer.  
  ResponseSpeed                  = 200  
  ComplexResponseSpeed           = 300  
  AdaptationOn                   = 0  
  PersistedBackgroundAdaptation  = 0  
  
Press any key to exit...  
```  
  
```csharp  
using System;  
using System.Globalization;  
using System.Speech.Recognition;  
  
namespace RecognizerSettings  
{  
  class Program  
  {  
    static readonly string[] settings = new string[] {  
      "ResourceUsage",  
      "ResponseSpeed",  
      "ComplexResponseSpeed",  
      "AdaptationOn",  
      "PersistedBackgroundAdaptation",  
    };  
  
    static void Main(string[] args)  
    {  
      using (SpeechRecognitionEngine recognizer =  
        new SpeechRecognitionEngine(new System.Globalization.CultureInfo("en-US")))  
      {  
        Console.WriteLine("Settings for recognizer {0}:",  
          recognizer.RecognizerInfo.Name);  
        Console.WriteLine();  
  
        // List the current settings.  
        ListSettings(recognizer);  
  
        // Change some of the settings.  
        recognizer.UpdateRecognizerSetting("ResponseSpeed", 200);  
        recognizer.UpdateRecognizerSetting("ComplexResponseSpeed", 300);  
        recognizer.UpdateRecognizerSetting("AdaptationOn", 1);  
        recognizer.UpdateRecognizerSetting("PersistedBackgroundAdaptation", 0);  
  
        Console.WriteLine("Updated settings:");  
        Console.WriteLine();  
  
        // List the updated settings.  
        ListSettings(recognizer);  
      }  
  
      Console.WriteLine("Press any key to exit...");  
      Console.ReadKey();  
    }  
  
    private static void ListSettings(SpeechRecognitionEngine recognizer)  
    {  
      foreach (string setting in settings)  
      {  
        try  
        {  
          object value = recognizer.QueryRecognizerSetting(setting);  
          Console.WriteLine("  {0,-30} = {1}", setting, value);  
        }  
        catch  
        {  
          Console.WriteLine("  {0,-30} is not supported by this recognizer.",  
            setting);  
        }  
      }  
      Console.WriteLine();  
    }  
  }  
}  
  
```  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> é <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> é a cadeia de caracteres vazia ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">O reconhecedor não tem uma configuração com esse nome.</exception>
      </Docs>
    </Member>
    <Member MemberName="UpdateRecognizerSetting">
      <MemberSignature Language="C#" Value="public void UpdateRecognizerSetting (string settingName, string updatedValue);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance void UpdateRecognizerSetting(string settingName, string updatedValue) cil managed" />
      <MemberSignature Language="DocId" Value="M:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting(System.String,System.String)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>System.Speech</AssemblyName>
        <AssemblyVersion>4.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>System.Void</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="settingName" Type="System.String" />
        <Parameter Name="updatedValue" Type="System.String" />
      </Parameters>
      <Docs>
        <param name="settingName">O nome da configuração a ser atualizada.</param>
        <param name="updatedValue">O novo valor para a configuração.</param>
        <summary>Atualiza a configuração de mecanismo de reconhecimento de fala especificado com o valor de cadeia de caracteres especificada.</summary>
        <remarks>
          <format type="text/markdown"><![CDATA[  
  
## Remarks  
 Com exceção de `PersistedBackgroundAdaptation`, valores de propriedade definidos usando o <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> método permanecem em vigor apenas para a instância atual do <xref:System.Speech.Recognition.SpeechRecognitionEngine>, após o qual reverter as configurações padrão. Consulte <xref:System.Speech.Recognition.SpeechRecognitionEngine.UpdateRecognizerSetting%2A> para obter descrições de configurações com suporte.  
  
 ]]></format>
        </remarks>
        <exception cref="T:System.ArgumentNullException">
          <paramref name="settingName" /> é <see langword="null" />.</exception>
        <exception cref="T:System.ArgumentException">
          <paramref name="settingName" /> é a cadeia de caracteres vazia ("").</exception>
        <exception cref="T:System.Collections.Generic.KeyNotFoundException">O reconhecedor não tem uma configuração com esse nome.</exception>
      </Docs>
    </Member>
  </Members>
</Type>
